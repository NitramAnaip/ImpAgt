{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Louis Ravillon, Martin Piana\n",
    "Date: October 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing directly on CNRS database looks a bit complicated\n",
    "\n",
    "For now we'll just try parsing the Arxiv website and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_url = \"https://arxiv.org/search/?query={}&searchtype=title&abstracts=show&order=-announced_date_first&size=50&start={}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/pdf/2010.02866.pdf', 'https://arxiv.org/pdf/2010.02749.pdf', 'https://arxiv.org/pdf/2010.02715.pdf', 'https://arxiv.org/pdf/2010.02670.pdf', 'https://arxiv.org/pdf/2010.02576.pdf', 'https://arxiv.org/pdf/2010.02523.pdf', 'https://arxiv.org/pdf/2010.02374.pdf', 'https://arxiv.org/pdf/2010.02317.pdf', 'https://arxiv.org/pdf/2010.02213.pdf', 'https://arxiv.org/pdf/2010.02174.pdf', 'https://arxiv.org/pdf/2010.02087.pdf', 'https://arxiv.org/pdf/2010.02086.pdf', 'https://arxiv.org/pdf/2010.02011.pdf', 'https://arxiv.org/pdf/2010.01996.pdf', 'https://arxiv.org/pdf/2010.01976.pdf', 'https://arxiv.org/pdf/2010.01968.pdf', 'https://arxiv.org/pdf/2010.01711.pdf', 'https://arxiv.org/pdf/2010.01709.pdf', 'https://arxiv.org/pdf/2010.01668.pdf', 'https://arxiv.org/pdf/2010.01582.pdf', 'https://arxiv.org/pdf/2010.01431.pdf', 'https://arxiv.org/pdf/2010.01213.pdf', 'https://arxiv.org/pdf/2010.01163.pdf', 'https://arxiv.org/pdf/2010.01149.pdf', 'https://arxiv.org/pdf/2010.01030.pdf', 'https://arxiv.org/pdf/2010.00964.pdf', 'https://arxiv.org/pdf/2010.00892.pdf', 'https://arxiv.org/pdf/2010.00848.pdf', 'https://arxiv.org/pdf/2010.00826.pdf', 'https://arxiv.org/pdf/2010.00821.pdf', 'https://arxiv.org/pdf/2010.00661.pdf', 'https://arxiv.org/pdf/2010.00619.pdf', 'https://arxiv.org/pdf/2010.00536.pdf', 'https://arxiv.org/pdf/2010.00532.pdf', 'https://arxiv.org/pdf/2010.00509.pdf', 'https://arxiv.org/pdf/2010.00401.pdf', 'https://arxiv.org/pdf/2010.00399.pdf', 'https://arxiv.org/pdf/2010.00397.pdf', 'https://arxiv.org/pdf/2010.00391.pdf', 'https://arxiv.org/pdf/2010.00353.pdf', 'https://arxiv.org/pdf/2010.00350.pdf', 'https://arxiv.org/pdf/2010.00330.pdf', 'https://arxiv.org/pdf/2010.00134.pdf', 'https://arxiv.org/pdf/2010.00110.pdf', 'https://arxiv.org/pdf/2010.00072.pdf', 'https://arxiv.org/pdf/2010.00054.pdf', 'https://arxiv.org/pdf/2010.00041.pdf', 'https://arxiv.org/pdf/2009.14695.pdf', 'https://arxiv.org/pdf/2009.14623.pdf', 'https://arxiv.org/pdf/2009.14596.pdf']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# choose the query we want\n",
    "# careful: here we're just working on the first 50 results (the website presents them 50 by 50)\n",
    "# if we want more we just have to iterate on size adding 50 by 50\n",
    "\n",
    "\n",
    "            \n",
    "def get_pdf_urls(website_url, query, total_size):\n",
    "    \"\"\"\n",
    "    ARGS: \n",
    "         - total_size is the total amount of articles we have on the web page. we have to check the webpage beforehand\n",
    "        to know what it amounts to\n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for i in range (0, total_size, 50):\n",
    "        size = i\n",
    "\n",
    "        x = requests.get(website_url.format(query, size)).content\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(x, parser)\n",
    "\n",
    "        results = soup(\"p\", class_=\"list-title is-inline-block\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "                if \"pdf\" in a['href']:\n",
    "                    urls.append(a['href']+\".pdf\")\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(arxiv_url, \"machine+learning\", 50)\n",
    "print(urls)\n",
    "print(type(urls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the text\n",
    "\n",
    "Now that we have the pdf url we have two options: either use beautiful soup, see the web page as a html doc and get the texts from there; either download completely the pdf and use pdfplumber or something alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll try with beautiful soup\n",
    "\n",
    "x = requests.get(\"https://arxiv.org/pdf/2010.02866.pdf\").content\n",
    "parser = \"html.parser\"\n",
    "soup = BeautifulSoup(x, parser)\n",
    "print(soup)\n",
    "#results = soup.findall(\"span\", string = True, limit = 5)\n",
    "#print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with scrapping\n",
    "\n",
    "We'll try downloading it here first and then getting the text out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = \"/home/martin/Desktop/ImpAgt/test.pdf\"\n",
    "\n",
    "chunk_size = 4000\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2010.02866.pdf\"\n",
    "r = requests.get(url, stream=True)\n",
    "\n",
    "with open(destination, 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size):\n",
    "        fd.write(chunk)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    " - On bloque un peu dans notre scrapping sur la base de recherche du CNRS. On pense que c'est parce que le site est en dynamique donc il faut qu'on creuse un peu \n",
    " - on est parti sur un site plus simple: Arxiv\n",
    " - On arrive pas a récupérer directement les docs donc on passe par un chemin détourné: on download en bloc et on utililse un package pour ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping bigger data base: Google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.mdpi.com/327494', 'https://www.sciencedirect.com/science/article/pii/S0168169917314710', 'https://www.sciencedirect.com/science/article/pii/S0168169918304289', 'https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR019933', 'https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312', 'https://academic.oup.com/erae/article-abstract/47/3/849/5552525', 'https://www.mdpi.com/2072-4292/8/6/514', 'https://www.sciencedirect.com/science/article/pii/S0048969720338602', 'https://ieeexplore.ieee.org/abstract/document/8534558/', 'https://ieeexplore.ieee.org/abstract/document/7225403/', 'https://www.sciencedirect.com/science/article/pii/S016816991630117X', 'https://link.springer.com/article/10.1007/s11119-014-9372-7', 'https://www.sciencedirect.com/science/article/pii/S0168192315007467', 'https://ieeexplore.ieee.org/abstract/document/7838138/', 'https://link.springer.com/chapter/10.1007/978-981-13-7403-6_50', 'https://www.sciencedirect.com/science/article/pii/S0168169917308803', 'https://www.nature.com/articles/544S21a', 'https://www.sciencedirect.com/science/article/pii/S0168169917314588', 'https://www.frontiersin.org/articles/10.3389/fpls.2020.00290/full?report=reader', 'https://www.sciencedirect.com/science/article/pii/S2589721719300182', 'https://arxiv.org/abs/1907.10794', 'https://ieeexplore.ieee.org/abstract/document/7325900/', 'https://www.sciencedirect.com/science/article/pii/S0168169918306987', 'https://link.springer.com/chapter/10.1007/978-3-319-94779-2_21', 'https://www.mdpi.com/2072-4292/10/8/1217', 'https://ieeexplore.ieee.org/abstract/document/7566749/', 'http://www.academia.edu/download/58075474/AI_in_Agriculture.pdf', 'http://www.ijabe.org/index.php/ijabe/article/view/4475', 'https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87', 'https://www.sciencedirect.com/science/article/pii/S1110866520301110', 'https://link.springer.com/article/10.1007/s11119-018-09628-4', 'https://www.sciencedirect.com/science/article/pii/S1360138515002630', 'https://www.sciencedirect.com/science/article/pii/S0168169917301230', 'https://www.sciencedirect.com/science/article/pii/S0378377418312800', 'https://academic.oup.com/aepp/article-abstract/40/1/79/4863692', 'https://www.sciencedirect.com/science/article/pii/S0924271618302090', 'https://www.sciencedirect.com/science/article/pii/S0168169915003671', 'https://ieeexplore.ieee.org/abstract/document/8554406/', 'https://www.sciencedirect.com/science/article/pii/S0303243415000719', 'https://ieeexplore.ieee.org/abstract/document/8523943/', 'https://ieeexplore.ieee.org/abstract/document/7473819/', 'https://www.aeaweb.org/articles?id=10.1257/aer.p20151021', 'https://www.sciencedirect.com/science/article/pii/S0048969718324021', 'https://www.sciencedirect.com/science/article/pii/S0016706117321031', 'https://www.sciencedirect.com/science/article/pii/S0022169418303184', 'https://www.sciencedirect.com/science/article/pii/S0924271618302260', 'https://www.aeaweb.org/articles?id=10.1257/aer.p20171038', 'https://arxiv.org/abs/1806.06762', 'https://link.springer.com/article/10.1007/s11119-017-9527-4', 'https://link.springer.com/article/10.1186/s40537-017-0077-4']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "scholar_url = \"https://scholar.google.fr/scholar?start={}&q={}&hl=fr&as_sdt=0,5&as_ylo={}&as_yhi={}\"\n",
    "\n",
    "def get_pdf_urls(website_url, query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    ARGS: query: what we want in the research bar\n",
    "         \n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for i in range (0, 50, 10):\n",
    "        size = i\n",
    "\n",
    "        x = requests.get(website_url.format(size, query, start_date, end_date)).content\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(x, parser)\n",
    "\n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "\n",
    "                urls.append(a['href'])\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(scholar_url, \"machine+learning+agriculture\", 2015, 2020)\n",
    "print(urls)\n",
    "print(len(urls))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "https://link.springer.com/article/10.1186/s40537-017-0077-4\n",
      "<class 'bs4.element.ResultSet'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_abstracts(url):\n",
    "\n",
    "    x = requests.get(url).content\n",
    "    parser = \"html.parser\"\n",
    "    print(type(x))\n",
    "    soup = BeautifulSoup(x, parser)\n",
    "    print(type(soup))\n",
    "    #results = soup(\"div\", class_=re.compile(\"abstract\"))\n",
    "    results = soup(\"div\")\n",
    "    print(url)\n",
    "    print(type(results))\n",
    "    for result in results:\n",
    "        print(type(result))\n",
    "        #print(result.get_text())\n",
    "\n",
    "    return 0\n",
    "\n",
    "get_abstracts(urls[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a real mess to get the abstracts: half of the websites dont let you get there (protection from robots) and the other half are so randomly coded that you get a lot of info you don't want. Most pertinent might still be to download the available pdfs and find a way to access the abstract pdfs from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14/10 : We had a call with B Frank who helped us on the bypassing of security checks we're facing with google scholar and the websites located on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiley online li\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "http = urllib3.PoolManager()\n",
    "S=0\n",
    "for i in range (len(urls)):\n",
    "    r = http.request('GET', urls[i])\n",
    "     # transform byte information to string info\n",
    "    string = r.data.decode(\"utf-8\")\n",
    "    if \"doctype html\" not in str.lower(string):\n",
    "        print(str.lower(string[:15]))\n",
    "        print(i)\n",
    "        S+=1\n",
    "print(S)\n",
    "\n",
    "\n",
    "#it seems that using this method we get access to the html whereas previously we didn't\n",
    "# we still have one or 2 problems(apparentlu with captchas) but it represents a small percentage \n",
    "# of the articles apparently\n",
    "# you can check by uncommenting the lines below\n",
    "\"\"\"\n",
    "r = http.request('GET', urls[4])\n",
    " # transform byte information to string info\n",
    "string = r.data.decode(\"utf-8\")\n",
    "print(string)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'http' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-14cc4fbe70ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GET'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;31m# transform byte information to string info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'http' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "r = http.request('GET', urls[4])\n",
    "print(urls[4])\n",
    " # transform byte information to string info\n",
    "    \n",
    "string = r.data.decode(\"utf-8\")\n",
    "\n",
    "#print(string)\n",
    "byte_page = r.data\n",
    "\n",
    "\n",
    "\n",
    "parser = \"html.parser\"\n",
    "soup = BeautifulSoup(byte_page, parser)\n",
    "#print(soup)\n",
    "results = soup(\"h2\", class_=\"widget-header header-none header-compact-vertical\")\n",
    "#print(results)\n",
    "\"\"\"\n",
    "for result in results:\n",
    "    print(result.get_text())\n",
    "\n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "\n",
    "                urls.append(a['href'])\n",
    "    return urls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this example we have a pb: we are getting empty parts in the html. This is due to he dynamic website? We might need to use selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'geckodriver' executable needs to be in PATH. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/ImpAgt/env/lib/python3.6/site-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                             stdin=PIPE)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'geckodriver': 'geckodriver'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-38c4c7dd5fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# start web browser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbrowser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFirefox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# get source code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ImpAgt/env/lib/python3.6/site-packages/selenium/webdriver/firefox/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, firefox_profile, firefox_binary, timeout, capabilities, proxy, executable_path, options, service_log_path, firefox_options, service_args, desired_capabilities, log_path, keep_alive)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 log_path=service_log_path)\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mcapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_capabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ImpAgt/env/lib/python3.6/site-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[1;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m---> 83\u001b[0;31m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[1;32m     84\u001b[0m                 )\n\u001b[1;32m     85\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: 'geckodriver' executable needs to be in PATH. \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# start web browser\n",
    "browser=webdriver.Firefox()\n",
    "\n",
    "# get source code\n",
    "browser.get('https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312')\n",
    "html = browser.page_source\n",
    "\n",
    "print(html)\n",
    "\n",
    "# close web browser\n",
    "browser.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check https://stackoverflow.com/questions/40208051/selenium-using-python-geckodriver-executable-needs-to-be-in-path and https://pythonbasics.org/selenium-get-html/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
