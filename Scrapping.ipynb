{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Louis Ravillon, Martin Piana\n",
    "Date: October 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n    <head>\\n        <title>BibCnrs</title>\\n        <meta charset=\"utf-8\"/>\\n        <meta name=\"description\" content=\"Portail BibCnrs IST\">\\n        <meta name=\"apple-mobile-web-app-capable\" content=\"yes\" />\\n        <meta name=\"robots\" content=\"index follow\">\\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, shrink-to-fit=no\">\\n        <link href=\"https://fonts.googleapis.com/css?family=DM+Sans&display=swap\" rel=\"stylesheet\">            <link rel=\"stylesheet\" media=\"all\" href=\"https://bib.cnrs.fr/wp-content/themes/portail/style.css?v=2.2\" type=\"text/css\"><link rel=\\'dns-prefetch\\' href=\\'//bib.cnrs.fr\\' />\\n<link rel=\\'dns-prefetch\\' href=\\'//s.w.org\\' />\\n<link rel=\"alternate\" type=\"application/rss+xml\" title=\"BibCnrs &raquo; Home Comments Feed\" href=\"https://bib.cnrs.fr/home/feed/\" />\\n\\t\\t<script type=\"text/javascript\">\\n\\t\\t\\twindow._wpemojiSettings = {\"baseUrl\":\"https:\\\\/\\\\/s.w.org\\\\/images\\\\/core\\\\/emoji\\\\/13.0.0\\\\/72x72\\\\/\",\"ext\":\".png\",\"svgUrl\":\"https:\\\\/\\\\/s.w.org\\\\/images\\\\/core\\\\/emoji\\\\/13.0.0\\\\/svg\\\\/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"https:\\\\/\\\\/bib.cnrs.fr\\\\/wp-includes\\\\/js\\\\/wp-emoji-release.min.js?ver=5.5.1\"}};\\n\\t\\t\\t!function(e,a,t){var r,n,o,i,p=a.createElement(\"canvas\"),s=p.getContext&&p.getContext(\"2d\");function c(e,t){var a=String.fromCharCode;s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,e),0,0);var r=p.toDataURL();return s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,t),0,0),r===p.toDataURL()}function l(e){if(!s||!s.fillText)return!1;switch(s.textBaseline=\"top\",s.font=\"600 32px Arial\",e){case\"flag\":return!c([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])&&(!c([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!c([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]));case\"emoji\":return!c([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}function d(e){var t=a.createElement(\"script\");t.src=e,t.defer=t.type=\"text/javascript\",a.getElementsByTagName(\"head\")[0].appendChild(t)}for(i=Array(\"flag\",\"emoji\"),t.supports={everything:!0,everythingExceptFlag:!0},o=0;o<i.length;o++)t.supports[i[o]]=l(i[o]),t.supports.everything=t.supports.everything&&t.supports[i[o]],\"flag\"!==i[o]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[i[o]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener(\"DOMContentLoaded\",n,!1),e.addEventListener(\"load\",n,!1)):(e.attachEvent(\"onload\",n),a.attachEvent(\"onreadystatechange\",function(){\"complete\"===a.readyState&&t.readyCallback()})),(r=t.source||{}).concatemoji?d(r.concatemoji):r.wpemoji&&r.twemoji&&(d(r.twemoji),d(r.wpemoji)))}(window,document,window._wpemojiSettings);\\n\\t\\t</script>\\n\\t\\t<style type=\"text/css\">\\nimg.wp-smiley,\\nimg.emoji {\\n\\tdisplay: inline !important;\\n\\tborder: none !important;\\n\\tbox-shadow: none !important;\\n\\theight: 1em !important;\\n\\twidth: 1em !important;\\n\\tmargin: 0 .07em !important;\\n\\tvertical-align: -0.1em !important;\\n\\tbackground: none !important;\\n\\tpadding: 0 !important;\\n}\\n</style>\\n\\t<link rel=\\'stylesheet\\' id=\\'wp-block-library-css\\'  href=\\'https://bib.cnrs.fr/wp-includes/css/dist/block-library/style.min.css?ver=5.5.1\\' type=\\'text/css\\' media=\\'all\\' />\\n<link rel=\\'stylesheet\\' id=\\'contact-form-7-css\\'  href=\\'https://bib.cnrs.fr/wp-content/plugins/contact-form-7/includes/css/styles.css?ver=5.2.2\\' type=\\'text/css\\' media=\\'all\\' />\\n<link rel=\\'stylesheet\\' id=\\'cookie-notice-front-css\\'  href=\\'https://bib.cnrs.fr/wp-content/plugins/cookie-notice/css/front.min.css?ver=5.5.1\\' type=\\'text/css\\' media=\\'all\\' />\\n<link rel=\\'stylesheet\\' id=\\'addtoany-css\\'  href=\\'https://bib.cnrs.fr/wp-content/plugins/add-to-any/addtoany.min.css?ver=1.15\\' type=\\'text/css\\' media=\\'all\\' />\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-includes/js/jquery/jquery.js?ver=1.12.4-wp\\' id=\\'jquery-core-js\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-content/plugins/add-to-any/addtoany.min.js?ver=1.1\\' id=\\'addtoany-js\\'></script>\\n<script type=\\'text/javascript\\' id=\\'cookie-notice-front-js-extra\\'>\\n/* <![CDATA[ */\\nvar cnArgs = {\"ajaxUrl\":\"https:\\\\/\\\\/bib.cnrs.fr\\\\/wp-admin\\\\/admin-ajax.php\",\"nonce\":\"0bffba47f6\",\"hideEffect\":\"fade\",\"position\":\"bottom\",\"onScroll\":\"0\",\"onScrollOffset\":\"100\",\"onClick\":\"0\",\"cookieName\":\"cookie_notice_accepted\",\"cookieTime\":\"2592000\",\"cookieTimeRejected\":\"2592000\",\"cookiePath\":\"\\\\/\",\"cookieDomain\":\"\",\"redirection\":\"0\",\"cache\":\"0\",\"refuse\":\"0\",\"revokeCookies\":\"0\",\"revokeCookiesOpt\":\"automatic\",\"secure\":\"1\",\"coronabarActive\":\"0\"};\\n/* ]]> */\\n</script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-content/plugins/cookie-notice/js/front.min.js?ver=1.3.2\\' id=\\'cookie-notice-front-js\\'></script>\\n<link rel=\"https://api.w.org/\" href=\"https://bib.cnrs.fr/wp-json/\" /><link rel=\"alternate\" type=\"application/json\" href=\"https://bib.cnrs.fr/wp-json/wp/v2/pages/324\" /><link rel=\"EditURI\" type=\"application/rsd+xml\" title=\"RSD\" href=\"https://bib.cnrs.fr/xmlrpc.php?rsd\" />\\n<link rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\" href=\"https://bib.cnrs.fr/wp-includes/wlwmanifest.xml\" /> \\n<meta name=\"generator\" content=\"WordPress 5.5.1\" />\\n<link rel=\"canonical\" href=\"https://bib.cnrs.fr/home/\" />\\n<link rel=\\'shortlink\\' href=\\'https://bib.cnrs.fr/\\' />\\n<link rel=\"alternate\" type=\"application/json+oembed\" href=\"https://bib.cnrs.fr/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fbib.cnrs.fr%2Fhome%2F\" />\\n<link rel=\"alternate\" type=\"text/xml+oembed\" href=\"https://bib.cnrs.fr/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fbib.cnrs.fr%2Fhome%2F&#038;format=xml\" />\\n\\n<script data-cfasync=\"false\">\\nwindow.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};a2a_localize = {\\n\\tShare: \"Share\",\\n\\tSave: \"Save\",\\n\\tSubscribe: \"Subscribe\",\\n\\tEmail: \"Email\",\\n\\tBookmark: \"Bookmark\",\\n\\tShowAll: \"Show all\",\\n\\tShowLess: \"Show less\",\\n\\tFindServices: \"Find service(s)\",\\n\\tFindAnyServiceToAddTo: \"Instantly find any service to add to\",\\n\\tPoweredBy: \"Powered by\",\\n\\tShareViaEmail: \"Share via email\",\\n\\tSubscribeViaEmail: \"Subscribe via email\",\\n\\tBookmarkInYourBrowser: \"Bookmark in your browser\",\\n\\tBookmarkInstructions: \"Press Ctrl+D or \\\\u2318+D to bookmark this page\",\\n\\tAddToYourFavorites: \"Add to your favorites\",\\n\\tSendFromWebOrProgram: \"Send from any email address or email program\",\\n\\tEmailProgram: \"Email program\",\\n\\tMore: \"More&#8230;\",\\n\\tThanksForSharing: \"Thanks for sharing!\",\\n\\tThanksForFollowing: \"Thanks for following!\"\\n};\\n\\n(function(d,s,a,b){a=d.createElement(s);b=d.getElementsByTagName(s)[0];a.async=1;a.src=\"https://static.addtoany.com/menu/page.js\";b.parentNode.insertBefore(a,b);})(document,\"script\");\\n</script>\\n<link rel=\"alternate\" href=\"https://bib.cnrs.fr/\" hreflang=\"fr\" />\\n<link rel=\"alternate\" href=\"https://bib.cnrs.fr/home/\" hreflang=\"en\" />\\n        <link rel=\"shortcut icon\" href=\"https://bib.cnrs.fr/wp-content/themes/portail/images/favicon.ico\">\\n    </head>\\n    <body>        <header class=\"flexrow\">            <div class=\"headerLeft\">\\n                <a href=\"/\">\\n                    <img\\n                        src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/bibcnrs-logo-blanc.png\"\\n                        class=\"bibcnrslogo\"\\n                        alt=\"logo bibcnrs\"\\n                    />\\n                </a>\\n                <div class=\"sitetitle bsbb\">\\n                    <div class=\"smalltitle\">CNRS documents access</div>\\n                </div>\\n            </div>\\n            <div class=\"headerRight\"><div id=\"ebsco_widget_header\">\\n</div>\\n\\n                <div class=\"language\"><select name=\"lang_choice_1\" id=\"lang_choice_1\">\\n\\t<option value=\"fr\">Fran\\xc3\\xa7ais</option>\\n\\t<option value=\"en\" selected=\\'selected\\'>English</option>\\n\\n</select>\\n<script type=\"text/javascript\">\\n\\t\\t\\t\\t\\t//<![CDATA[\\n\\t\\t\\t\\t\\tvar urls_1 = {\"fr\":\"https:\\\\/\\\\/bib.cnrs.fr\\\\/\",\"en\":\"https:\\\\/\\\\/bib.cnrs.fr\\\\/home\\\\/\"};\\n\\t\\t\\t\\t\\tdocument.getElementById( \"lang_choice_1\" ).onchange = function() {\\n\\t\\t\\t\\t\\t\\tlocation.href = urls_1[this.value];\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t//]]>\\n\\t\\t\\t\\t</script>\\n                </div>\\n            </div>\\n        </header>        <div class=\"main\"><div id=\"ebsco_widget\">\\n    <div class=\"ebsco-widget-loader\"></div>\\n</div>\\n    <div class=\"alerte\"><p style=\"text-align: center;\"><span style=\"color: #993366;\"><a href=\"https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1.png\"><img loading=\"lazy\" class=\"wp-image-12854 alignleft\" src=\"https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1-150x150.png\" alt=\"\" width=\"66\" height=\"66\" srcset=\"https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1.png 150w, https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1-100x100.png 100w\" sizes=\"(max-width: 66px) 100vw, 66px\" /></a></span></p>\\n<p>&nbsp;</p>\\n<p><span style=\"color: #993366;\">\\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 Connection required before any search !\\xc2\\xa0 \\xc2\\xa0</span>\\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0 \\xc2\\xa0<span style=\"color: #993366;\">\\xc2\\xa0</span></p>\\n<p>&nbsp;</p>\\n<p style=\"text-align: center;\">\\n</div>        </div>\\n        <div id=\"idRepeat\">\\n            <div id=\"plusTexte\">More of BibCnrs\\n            </div>\\n            <div id=\"plusBibCnrs\">\\n                <div class=\"homeLink\">                        <a href=\"/category/discover_bibcnrs\">                        <img src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/Decouvrir_190@2x.png\" alt=\"d\\xc3\\xa9coration\">\\n                        <div class=\"homeLinkText\">Discover BibCnrs\\n                        </div>\\n                    </a>\\n                </div>\\n                <div class=\"homeLink\">                        <a href=\"/category/tools\">                        <img src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/Integrer_190@2x.png\" alt=\"d\\xc3\\xa9coration\">\\n                        <div class=\"homeLinkText\">Tools\\n                        </div>\\n                    </a>\\n                </div>\\n                 <div class=\"homeLink\">                        <a href=\"/category/news\">                        <img src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/News_190@2x.png\" alt=\"d\\xc3\\xa9coration\">\\n                        <div class=\"homeLinkText\">News /Tests\\n                        </div>\\n                    </a>\\n                </div>\\n           </div>\\n        </div>            <footer>\\n                <a href=\"http://www.cnrs.fr\" target=\"_blank\" >\\n                    <img src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/logo_cnrs_80x80.png\" alt=\"logo cnrs\" class=\"logocnrs\"/>\\n                </a>                    <ul class=\"menu-bas-fr menu-bas-home\">                        <li class=\"nav-main-item menu-item menu-item-type-post_type menu-item-object-page menu-item-601\">\\n                            <a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/about/\">About\\n                            </a>\\n                        </li>                        <li class=\"nav-main-item menu-item menu-item-type-taxonomy menu-item-object-category menu-item-359\">\\n                            <a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/category/faq-en/\">FAQ\\n                            </a>\\n                        </li>                        <li class=\"nav-main-item menu-item menu-item-type-post_type menu-item-object-page menu-item-358\">\\n                            <a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/contact-3/\">Contact\\n                            </a>\\n                        </li>                        <li class=\"nav-main-item menu-item menu-item-type-post_type menu-item-object-page menu-item-privacy-policy menu-item-2159\">\\n                            <a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/legal-notice/\">Legal notice\\n                            </a>\\n                        </li>                </ul>\\n                <div id=\"socialMedia\">\\n                    <a href=\"https://twitter.com/BibCnrs\" title=\"twitter\" target=\"_blank\">\\n                    <svg width=\"29px\" height=\"26px\" viewBox=\"0 0 22 18\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\\n                        <title>twitter</title>\\n                        <path fill=\"#fff\" d=\"M21.9786386,2.12515726 C21.1711619,2.48592871 20.3017782,2.72918586 19.3895457,2.83847158 C20.3205805,2.27815727 21.0341735,1.3894715 21.3723595,0.330942875 C20.5002898,0.85088576 19.5344643,1.22785721 18.5081387,1.43061436 C17.6858248,0.550028601 16.5134295,0 15.217604,0 C12.7266157,0 10.7076042,2.02860011 10.7076042,4.53201453 C10.7076042,4.88725741 10.747767,5.23324314 10.8243833,5.56495744 C7.07773225,5.37608601 3.75343007,3.57197162 1.53066272,0.829285759 C1.14297669,1.49875722 0.920162751,2.27751441 0.920162751,3.10821445 C0.920162751,4.68038597 1.71574411,6.0679289 2.92574406,6.8803718 C2.18669758,6.85748609 1.4916511,6.65318607 0.882686008,6.31337177 L0.882686008,6.37071463 C0.882686008,8.56658618 2.43880222,10.398472 4.49938353,10.8150434 C4.12231377,10.9189292 3.72388356,10.9740863 3.31215102,10.9740863 C3.02090685,10.9740863 2.7392557,10.9458006 2.46438361,10.892572 C3.03702312,12.6934721 4.70326724,14.0042579 6.67508111,14.040515 C5.13239513,15.256158 3.18999986,15.9802723 1.07582553,15.9802723 C0.712313923,15.9802723 0.354174403,15.9594437 0,15.9175294 C1.99598829,17.2032438 4.36508121,17.9535867 6.91260435,17.9535867 C15.2068598,17.9535867 19.742441,11.0471149 19.742441,5.05800027 C19.742441,4.86102883 19.7382201,4.66560025 19.7290108,4.47145739 C20.6102898,3.83220021 21.3750456,3.03390016 21.9786386,2.12515726\" id=\"Fill-1\"></path>\\n                    </svg>\\n                    </a>\\n                </div>\\n            </footer><link rel=\\'stylesheet\\' id=\\'ebsco_widget-css\\'  href=\\'https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/node_modules/ebsco-widget/build/app.css?ver=2.14.3\\' type=\\'text/css\\' media=\\'all\\' />\\n<link rel=\\'stylesheet\\' id=\\'ebsco_widget-loader-css\\'  href=\\'https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/css/loader.css?ver=5.5.1\\' type=\\'text/css\\' media=\\'all\\' />\\n<script type=\\'text/javascript\\' id=\\'contact-form-7-js-extra\\'>\\n/* <![CDATA[ */\\nvar wpcf7 = {\"apiSettings\":{\"root\":\"https:\\\\/\\\\/bib.cnrs.fr\\\\/wp-json\\\\/contact-form-7\\\\/v1\",\"namespace\":\"contact-form-7\\\\/v1\"}};\\n/* ]]> */\\n</script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-content/plugins/contact-form-7/includes/js/scripts.js?ver=5.2.2\\' id=\\'contact-form-7-js\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-includes/js/wp-embed.min.js?ver=5.5.1\\' id=\\'wp-embed-js\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/node_modules/babel-polyfill/dist/polyfill.min.js?ver=6.26.0\\' id=\\'babel-polyfill-js\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill.min.js?ver=7.4.4\\' id=\\'wp-polyfill-js\\'></script>\\n<script type=\\'text/javascript\\' id=\\'wp-polyfill-js-after\\'>\\n( \\'fetch\\' in window ) || document.write( \\'<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-fetch.min.js?ver=3.0.0\"></scr\\' + \\'ipt>\\' );( document.contains ) || document.write( \\'<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-node-contains.min.js?ver=3.42.0\"></scr\\' + \\'ipt>\\' );( window.DOMRect ) || document.write( \\'<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-dom-rect.min.js?ver=3.42.0\"></scr\\' + \\'ipt>\\' );( window.URL && window.URL.prototype && window.URLSearchParams ) || document.write( \\'<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-url.min.js?ver=3.6.4\"></scr\\' + \\'ipt>\\' );( window.FormData && window.FormData.prototype.keys ) || document.write( \\'<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-formdata.min.js?ver=3.0.12\"></scr\\' + \\'ipt>\\' );( Element.prototype.matches && Element.prototype.closest ) || document.write( \\'<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-element-closest.min.js?ver=2.0.2\"></scr\\' + \\'ipt>\\' );\\n</script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-includes/js/dist/vendor/react.min.js?ver=16.9.0\\' id=\\'react-js\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-includes/js/dist/vendor/react-dom.min.js?ver=16.9.0\\' id=\\'react-dom-js\\'></script>\\n<script type=\\'text/javascript\\' src=\\'https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/node_modules/ebsco-widget/build/app.js?ver=2.14.3\\' id=\\'ebsco_widget-js\\'></script>\\n<script type=\\'text/javascript\\' id=\"ebsco_widget-index\" data-url=\"https://bib.cnrs.fr/api/ebsco\" data-domain=\"\" data-language=\"en\" data-publication_sort=\"0\" src=\\'https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/javascripts/index.js?ver=3.15.15\\' id=\\'ebsco_widget-index-js\\'></script>\\n\\r\\n\\t\\t<!-- Cookie Notice plugin v1.3.2 by Digital Factory https://dfactory.eu/ -->\\r\\n\\t\\t<div id=\"cookie-notice\" role=\"banner\" class=\"cookie-notice-hidden cookie-revoke-hidden cn-position-bottom\" aria-label=\"Cookie Notice\" style=\"background-color: rgba(0,0,0,1);\"><div class=\"cookie-notice-container\" style=\"color: #fff;\"><span id=\"cn-notice-text\" class=\"cn-text-container\">We use cookies to ensure that we give you the best experience on our website. If you continue to use this site we will assume that you are happy with it.</span><span id=\"cn-notice-buttons\" class=\"cn-buttons-container\"><a href=\"#\" id=\"cn-accept-cookie\" data-cookie-set=\"accept\" class=\"cn-set-cookie cn-button bootstrap button\" aria-label=\"Ok\">Ok</a></span><a href=\"javascript:void(0);\" id=\"cn-close-notice\" data-cookie-set=\"accept\" class=\"cn-close-icon\" aria-label=\"Ok\"></a></div>\\r\\n\\t\\t\\t\\r\\n\\t\\t</div>\\r\\n\\t\\t<!-- / Cookie Notice plugin -->\\n        <!-- Piwik -->\\n        <script type=\"text/javascript\">\\n            var _paq = _paq || [];\\n            _paq.push([\\'trackPageView\\']);\\n            _paq.push([\\'enableLinkTracking\\']);\\n            (function() {\\n                var u=\"//piwik2.inist.fr/\";\\n                _paq.push([\\'setTrackerUrl\\', u+\\'piwik.php\\']);\\n                _paq.push([\\'setSiteId\\', \\'1\\']);\\n                var d=document,\\n                    g=d.createElement(\\'script\\'),\\n                    s=d.getElementsByTagName(\\'script\\')[0];\\n                g.type=\\'text/javascript\\';\\n                g.async=true;\\n                g.defer=true;\\n                g.src=u+\\'piwik.js\\';\\n                s.parentNode.insertBefore(g,s);\\n            })();\\n        </script>\\n        <noscript>\\n            <p>\\n                <img src=\"//piwik2.inist.fr/piwik.php?idsite=1\" style=\"border:0;\" alt=\"\" />\\n            </p>\\n        </noscript>\\n        <!-- End Piwik Code -->\\n    </body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "x=requests.get(\"https://bib.cnrs.fr/home/\").content\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>BibCnrs</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"Portail BibCnrs IST\" name=\"description\"/>\n",
      "<meta content=\"yes\" name=\"apple-mobile-web-app-capable\">\n",
      "<meta content=\"index follow\" name=\"robots\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1.0, shrink-to-fit=no\" name=\"viewport\"/>\n",
      "<link href=\"https://fonts.googleapis.com/css?family=DM+Sans&amp;display=swap\" rel=\"stylesheet\"/> <link href=\"https://bib.cnrs.fr/wp-content/themes/portail/style.css?v=2.2\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/><link href=\"//bib.cnrs.fr\" rel=\"dns-prefetch\">\n",
      "<link href=\"//s.w.org\" rel=\"dns-prefetch\">\n",
      "<link href=\"https://bib.cnrs.fr/home/feed/\" rel=\"alternate\" title=\"BibCnrs » Home Comments Feed\" type=\"application/rss+xml\"/>\n",
      "<script type=\"text/javascript\">\n",
      "\t\t\twindow._wpemojiSettings = {\"baseUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/13.0.0\\/72x72\\/\",\"ext\":\".png\",\"svgUrl\":\"https:\\/\\/s.w.org\\/images\\/core\\/emoji\\/13.0.0\\/svg\\/\",\"svgExt\":\".svg\",\"source\":{\"concatemoji\":\"https:\\/\\/bib.cnrs.fr\\/wp-includes\\/js\\/wp-emoji-release.min.js?ver=5.5.1\"}};\n",
      "\t\t\t!function(e,a,t){var r,n,o,i,p=a.createElement(\"canvas\"),s=p.getContext&&p.getContext(\"2d\");function c(e,t){var a=String.fromCharCode;s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,e),0,0);var r=p.toDataURL();return s.clearRect(0,0,p.width,p.height),s.fillText(a.apply(this,t),0,0),r===p.toDataURL()}function l(e){if(!s||!s.fillText)return!1;switch(s.textBaseline=\"top\",s.font=\"600 32px Arial\",e){case\"flag\":return!c([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])&&(!c([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!c([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]));case\"emoji\":return!c([55357,56424,8205,55356,57212],[55357,56424,8203,55356,57212])}return!1}function d(e){var t=a.createElement(\"script\");t.src=e,t.defer=t.type=\"text/javascript\",a.getElementsByTagName(\"head\")[0].appendChild(t)}for(i=Array(\"flag\",\"emoji\"),t.supports={everything:!0,everythingExceptFlag:!0},o=0;o<i.length;o++)t.supports[i[o]]=l(i[o]),t.supports.everything=t.supports.everything&&t.supports[i[o]],\"flag\"!==i[o]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[i[o]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener(\"DOMContentLoaded\",n,!1),e.addEventListener(\"load\",n,!1)):(e.attachEvent(\"onload\",n),a.attachEvent(\"onreadystatechange\",function(){\"complete\"===a.readyState&&t.readyCallback()})),(r=t.source||{}).concatemoji?d(r.concatemoji):r.wpemoji&&r.twemoji&&(d(r.twemoji),d(r.wpemoji)))}(window,document,window._wpemojiSettings);\n",
      "\t\t</script>\n",
      "<style type=\"text/css\">\n",
      "img.wp-smiley,\n",
      "img.emoji {\n",
      "\tdisplay: inline !important;\n",
      "\tborder: none !important;\n",
      "\tbox-shadow: none !important;\n",
      "\theight: 1em !important;\n",
      "\twidth: 1em !important;\n",
      "\tmargin: 0 .07em !important;\n",
      "\tvertical-align: -0.1em !important;\n",
      "\tbackground: none !important;\n",
      "\tpadding: 0 !important;\n",
      "}\n",
      "</style>\n",
      "<link href=\"https://bib.cnrs.fr/wp-includes/css/dist/block-library/style.min.css?ver=5.5.1\" id=\"wp-block-library-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-content/plugins/contact-form-7/includes/css/styles.css?ver=5.2.2\" id=\"contact-form-7-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-content/plugins/cookie-notice/css/front.min.css?ver=5.5.1\" id=\"cookie-notice-front-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-content/plugins/add-to-any/addtoany.min.css?ver=1.15\" id=\"addtoany-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<script id=\"jquery-core-js\" src=\"https://bib.cnrs.fr/wp-includes/js/jquery/jquery.js?ver=1.12.4-wp\" type=\"text/javascript\"></script>\n",
      "<script id=\"addtoany-js\" src=\"https://bib.cnrs.fr/wp-content/plugins/add-to-any/addtoany.min.js?ver=1.1\" type=\"text/javascript\"></script>\n",
      "<script id=\"cookie-notice-front-js-extra\" type=\"text/javascript\">\n",
      "/* <![CDATA[ */\n",
      "var cnArgs = {\"ajaxUrl\":\"https:\\/\\/bib.cnrs.fr\\/wp-admin\\/admin-ajax.php\",\"nonce\":\"0bffba47f6\",\"hideEffect\":\"fade\",\"position\":\"bottom\",\"onScroll\":\"0\",\"onScrollOffset\":\"100\",\"onClick\":\"0\",\"cookieName\":\"cookie_notice_accepted\",\"cookieTime\":\"2592000\",\"cookieTimeRejected\":\"2592000\",\"cookiePath\":\"\\/\",\"cookieDomain\":\"\",\"redirection\":\"0\",\"cache\":\"0\",\"refuse\":\"0\",\"revokeCookies\":\"0\",\"revokeCookiesOpt\":\"automatic\",\"secure\":\"1\",\"coronabarActive\":\"0\"};\n",
      "/* ]]> */\n",
      "</script>\n",
      "<script id=\"cookie-notice-front-js\" src=\"https://bib.cnrs.fr/wp-content/plugins/cookie-notice/js/front.min.js?ver=1.3.2\" type=\"text/javascript\"></script>\n",
      "<link href=\"https://bib.cnrs.fr/wp-json/\" rel=\"https://api.w.org/\"/><link href=\"https://bib.cnrs.fr/wp-json/wp/v2/pages/324\" rel=\"alternate\" type=\"application/json\"/><link href=\"https://bib.cnrs.fr/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n",
      "<meta content=\"WordPress 5.5.1\" name=\"generator\">\n",
      "<link href=\"https://bib.cnrs.fr/home/\" rel=\"canonical\"/>\n",
      "<link href=\"https://bib.cnrs.fr/\" rel=\"shortlink\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fbib.cnrs.fr%2Fhome%2F\" rel=\"alternate\" type=\"application/json+oembed\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-json/oembed/1.0/embed?url=https%3A%2F%2Fbib.cnrs.fr%2Fhome%2F&amp;format=xml\" rel=\"alternate\" type=\"text/xml+oembed\"/>\n",
      "<script data-cfasync=\"false\">\n",
      "window.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};a2a_localize = {\n",
      "\tShare: \"Share\",\n",
      "\tSave: \"Save\",\n",
      "\tSubscribe: \"Subscribe\",\n",
      "\tEmail: \"Email\",\n",
      "\tBookmark: \"Bookmark\",\n",
      "\tShowAll: \"Show all\",\n",
      "\tShowLess: \"Show less\",\n",
      "\tFindServices: \"Find service(s)\",\n",
      "\tFindAnyServiceToAddTo: \"Instantly find any service to add to\",\n",
      "\tPoweredBy: \"Powered by\",\n",
      "\tShareViaEmail: \"Share via email\",\n",
      "\tSubscribeViaEmail: \"Subscribe via email\",\n",
      "\tBookmarkInYourBrowser: \"Bookmark in your browser\",\n",
      "\tBookmarkInstructions: \"Press Ctrl+D or \\u2318+D to bookmark this page\",\n",
      "\tAddToYourFavorites: \"Add to your favorites\",\n",
      "\tSendFromWebOrProgram: \"Send from any email address or email program\",\n",
      "\tEmailProgram: \"Email program\",\n",
      "\tMore: \"More&#8230;\",\n",
      "\tThanksForSharing: \"Thanks for sharing!\",\n",
      "\tThanksForFollowing: \"Thanks for following!\"\n",
      "};\n",
      "\n",
      "(function(d,s,a,b){a=d.createElement(s);b=d.getElementsByTagName(s)[0];a.async=1;a.src=\"https://static.addtoany.com/menu/page.js\";b.parentNode.insertBefore(a,b);})(document,\"script\");\n",
      "</script>\n",
      "<link href=\"https://bib.cnrs.fr/\" hreflang=\"fr\" rel=\"alternate\"/>\n",
      "<link href=\"https://bib.cnrs.fr/home/\" hreflang=\"en\" rel=\"alternate\"/>\n",
      "<link href=\"https://bib.cnrs.fr/wp-content/themes/portail/images/favicon.ico\" rel=\"shortcut icon\"/>\n",
      "</meta></link></link></meta></head>\n",
      "<body> <header class=\"flexrow\"> <div class=\"headerLeft\">\n",
      "<a href=\"/\">\n",
      "<img alt=\"logo bibcnrs\" class=\"bibcnrslogo\" src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/bibcnrs-logo-blanc.png\"/>\n",
      "</a>\n",
      "<div class=\"sitetitle bsbb\">\n",
      "<div class=\"smalltitle\">CNRS documents access</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"headerRight\"><div id=\"ebsco_widget_header\">\n",
      "</div>\n",
      "<div class=\"language\"><select id=\"lang_choice_1\" name=\"lang_choice_1\">\n",
      "<option value=\"fr\">Français</option>\n",
      "<option selected=\"selected\" value=\"en\">English</option>\n",
      "</select>\n",
      "<script type=\"text/javascript\">\n",
      "\t\t\t\t\t//<![CDATA[\n",
      "\t\t\t\t\tvar urls_1 = {\"fr\":\"https:\\/\\/bib.cnrs.fr\\/\",\"en\":\"https:\\/\\/bib.cnrs.fr\\/home\\/\"};\n",
      "\t\t\t\t\tdocument.getElementById( \"lang_choice_1\" ).onchange = function() {\n",
      "\t\t\t\t\t\tlocation.href = urls_1[this.value];\n",
      "\t\t\t\t\t}\n",
      "\t\t\t\t\t//]]>\n",
      "\t\t\t\t</script>\n",
      "</div>\n",
      "</div>\n",
      "</header> <div class=\"main\"><div id=\"ebsco_widget\">\n",
      "<div class=\"ebsco-widget-loader\"></div>\n",
      "</div>\n",
      "<div class=\"alerte\"><p style=\"text-align: center;\"><span style=\"color: #993366;\"><a href=\"https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1.png\"><img alt=\"\" class=\"wp-image-12854 alignleft\" height=\"66\" loading=\"lazy\" sizes=\"(max-width: 66px) 100vw, 66px\" src=\"https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1-150x150.png\" srcset=\"https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1.png 150w, https://bib.cnrs.fr/wp-content/uploads/2020/06/Sans-titre.png_v2-150x150-1-100x100.png 100w\" width=\"66\"/></a></span></p>\n",
      "<p> </p>\n",
      "<p><span style=\"color: #993366;\">                      Connection required before any search !   </span>                         <span style=\"color: #993366;\"> </span></p>\n",
      "<p> </p>\n",
      "<p style=\"text-align: center;\">\n",
      "</p></div> </div>\n",
      "<div id=\"idRepeat\">\n",
      "<div id=\"plusTexte\">More of BibCnrs\n",
      "            </div>\n",
      "<div id=\"plusBibCnrs\">\n",
      "<div class=\"homeLink\"> <a href=\"/category/discover_bibcnrs\"> <img alt=\"décoration\" src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/Decouvrir_190@2x.png\"/>\n",
      "<div class=\"homeLinkText\">Discover BibCnrs\n",
      "                        </div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"homeLink\"> <a href=\"/category/tools\"> <img alt=\"décoration\" src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/Integrer_190@2x.png\"/>\n",
      "<div class=\"homeLinkText\">Tools\n",
      "                        </div>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"homeLink\"> <a href=\"/category/news\"> <img alt=\"décoration\" src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/News_190@2x.png\"/>\n",
      "<div class=\"homeLinkText\">News /Tests\n",
      "                        </div>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "</div> <footer>\n",
      "<a href=\"http://www.cnrs.fr\" target=\"_blank\">\n",
      "<img alt=\"logo cnrs\" class=\"logocnrs\" src=\"https://bib.cnrs.fr/wp-content/themes/portail/images/logo_cnrs_80x80.png\">\n",
      "</img></a> <ul class=\"menu-bas-fr menu-bas-home\"> <li class=\"nav-main-item menu-item menu-item-type-post_type menu-item-object-page menu-item-601\">\n",
      "<a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/about/\">About\n",
      "                            </a>\n",
      "</li> <li class=\"nav-main-item menu-item menu-item-type-taxonomy menu-item-object-category menu-item-359\">\n",
      "<a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/category/faq-en/\">FAQ\n",
      "                            </a>\n",
      "</li> <li class=\"nav-main-item menu-item menu-item-type-post_type menu-item-object-page menu-item-358\">\n",
      "<a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/contact-3/\">Contact\n",
      "                            </a>\n",
      "</li> <li class=\"nav-main-item menu-item menu-item-type-post_type menu-item-object-page menu-item-privacy-policy menu-item-2159\">\n",
      "<a class=\"nav-main-link\" href=\"https://bib.cnrs.fr/legal-notice/\">Legal notice\n",
      "                            </a>\n",
      "</li> </ul>\n",
      "<div id=\"socialMedia\">\n",
      "<a href=\"https://twitter.com/BibCnrs\" target=\"_blank\" title=\"twitter\">\n",
      "<svg height=\"26px\" version=\"1.1\" viewbox=\"0 0 22 18\" width=\"29px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
      "<title>twitter</title>\n",
      "<path d=\"M21.9786386,2.12515726 C21.1711619,2.48592871 20.3017782,2.72918586 19.3895457,2.83847158 C20.3205805,2.27815727 21.0341735,1.3894715 21.3723595,0.330942875 C20.5002898,0.85088576 19.5344643,1.22785721 18.5081387,1.43061436 C17.6858248,0.550028601 16.5134295,0 15.217604,0 C12.7266157,0 10.7076042,2.02860011 10.7076042,4.53201453 C10.7076042,4.88725741 10.747767,5.23324314 10.8243833,5.56495744 C7.07773225,5.37608601 3.75343007,3.57197162 1.53066272,0.829285759 C1.14297669,1.49875722 0.920162751,2.27751441 0.920162751,3.10821445 C0.920162751,4.68038597 1.71574411,6.0679289 2.92574406,6.8803718 C2.18669758,6.85748609 1.4916511,6.65318607 0.882686008,6.31337177 L0.882686008,6.37071463 C0.882686008,8.56658618 2.43880222,10.398472 4.49938353,10.8150434 C4.12231377,10.9189292 3.72388356,10.9740863 3.31215102,10.9740863 C3.02090685,10.9740863 2.7392557,10.9458006 2.46438361,10.892572 C3.03702312,12.6934721 4.70326724,14.0042579 6.67508111,14.040515 C5.13239513,15.256158 3.18999986,15.9802723 1.07582553,15.9802723 C0.712313923,15.9802723 0.354174403,15.9594437 0,15.9175294 C1.99598829,17.2032438 4.36508121,17.9535867 6.91260435,17.9535867 C15.2068598,17.9535867 19.742441,11.0471149 19.742441,5.05800027 C19.742441,4.86102883 19.7382201,4.66560025 19.7290108,4.47145739 C20.6102898,3.83220021 21.3750456,3.03390016 21.9786386,2.12515726\" fill=\"#fff\" id=\"Fill-1\"></path>\n",
      "</svg>\n",
      "</a>\n",
      "</div>\n",
      "</footer><link href=\"https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/node_modules/ebsco-widget/build/app.css?ver=2.14.3\" id=\"ebsco_widget-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\">\n",
      "<link href=\"https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/css/loader.css?ver=5.5.1\" id=\"ebsco_widget-loader-css\" media=\"all\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<script id=\"contact-form-7-js-extra\" type=\"text/javascript\">\n",
      "/* <![CDATA[ */\n",
      "var wpcf7 = {\"apiSettings\":{\"root\":\"https:\\/\\/bib.cnrs.fr\\/wp-json\\/contact-form-7\\/v1\",\"namespace\":\"contact-form-7\\/v1\"}};\n",
      "/* ]]> */\n",
      "</script>\n",
      "<script id=\"contact-form-7-js\" src=\"https://bib.cnrs.fr/wp-content/plugins/contact-form-7/includes/js/scripts.js?ver=5.2.2\" type=\"text/javascript\"></script>\n",
      "<script id=\"wp-embed-js\" src=\"https://bib.cnrs.fr/wp-includes/js/wp-embed.min.js?ver=5.5.1\" type=\"text/javascript\"></script>\n",
      "<script id=\"babel-polyfill-js\" src=\"https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/node_modules/babel-polyfill/dist/polyfill.min.js?ver=6.26.0\" type=\"text/javascript\"></script>\n",
      "<script id=\"wp-polyfill-js\" src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill.min.js?ver=7.4.4\" type=\"text/javascript\"></script>\n",
      "<script id=\"wp-polyfill-js-after\" type=\"text/javascript\">\n",
      "( 'fetch' in window ) || document.write( '<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-fetch.min.js?ver=3.0.0\"></scr' + 'ipt>' );( document.contains ) || document.write( '<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-node-contains.min.js?ver=3.42.0\"></scr' + 'ipt>' );( window.DOMRect ) || document.write( '<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-dom-rect.min.js?ver=3.42.0\"></scr' + 'ipt>' );( window.URL && window.URL.prototype && window.URLSearchParams ) || document.write( '<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-url.min.js?ver=3.6.4\"></scr' + 'ipt>' );( window.FormData && window.FormData.prototype.keys ) || document.write( '<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-formdata.min.js?ver=3.0.12\"></scr' + 'ipt>' );( Element.prototype.matches && Element.prototype.closest ) || document.write( '<script src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/wp-polyfill-element-closest.min.js?ver=2.0.2\"></scr' + 'ipt>' );\n",
      "</script>\n",
      "<script id=\"react-js\" src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/react.min.js?ver=16.9.0\" type=\"text/javascript\"></script>\n",
      "<script id=\"react-dom-js\" src=\"https://bib.cnrs.fr/wp-includes/js/dist/vendor/react-dom.min.js?ver=16.9.0\" type=\"text/javascript\"></script>\n",
      "<script id=\"ebsco_widget-js\" src=\"https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/node_modules/ebsco-widget/build/app.js?ver=2.14.3\" type=\"text/javascript\"></script>\n",
      "<script data-domain=\"\" data-language=\"en\" data-publication_sort=\"0\" data-url=\"https://bib.cnrs.fr/api/ebsco\" id=\"ebsco_widget-index-js\" src=\"https://bib.cnrs.fr/wp-content/plugins/wp-ebsco-widget/javascripts/index.js?ver=3.15.15\" type=\"text/javascript\"></script>\n",
      "<!-- Cookie Notice plugin v1.3.2 by Digital Factory https://dfactory.eu/ -->\n",
      "<div aria-label=\"Cookie Notice\" class=\"cookie-notice-hidden cookie-revoke-hidden cn-position-bottom\" id=\"cookie-notice\" role=\"banner\" style=\"background-color: rgba(0,0,0,1);\"><div class=\"cookie-notice-container\" style=\"color: #fff;\"><span class=\"cn-text-container\" id=\"cn-notice-text\">We use cookies to ensure that we give you the best experience on our website. If you continue to use this site we will assume that you are happy with it.</span><span class=\"cn-buttons-container\" id=\"cn-notice-buttons\"><a aria-label=\"Ok\" class=\"cn-set-cookie cn-button bootstrap button\" data-cookie-set=\"accept\" href=\"#\" id=\"cn-accept-cookie\">Ok</a></span><a aria-label=\"Ok\" class=\"cn-close-icon\" data-cookie-set=\"accept\" href=\"javascript:void(0);\" id=\"cn-close-notice\"></a></div>\n",
      "</div>\n",
      "<!-- / Cookie Notice plugin -->\n",
      "<!-- Piwik -->\n",
      "<script type=\"text/javascript\">\n",
      "            var _paq = _paq || [];\n",
      "            _paq.push(['trackPageView']);\n",
      "            _paq.push(['enableLinkTracking']);\n",
      "            (function() {\n",
      "                var u=\"//piwik2.inist.fr/\";\n",
      "                _paq.push(['setTrackerUrl', u+'piwik.php']);\n",
      "                _paq.push(['setSiteId', '1']);\n",
      "                var d=document,\n",
      "                    g=d.createElement('script'),\n",
      "                    s=d.getElementsByTagName('script')[0];\n",
      "                g.type='text/javascript';\n",
      "                g.async=true;\n",
      "                g.defer=true;\n",
      "                g.src=u+'piwik.js';\n",
      "                s.parentNode.insertBefore(g,s);\n",
      "            })();\n",
      "        </script>\n",
      "<noscript>\n",
      "<p>\n",
      "<img alt=\"\" src=\"//piwik2.inist.fr/piwik.php?idsite=1\" style=\"border:0;\">\n",
      "</img></p>\n",
      "</noscript>\n",
      "<!-- End Piwik Code -->\n",
      "</link></body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = \"html.parser\"\n",
    "soup = BeautifulSoup(x, parser)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing directly on CNRS database looks a bit complicated\n",
    "\n",
    "For now we'll just try parsing the Arxiv website and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_url = \"https://arxiv.org/search/?query={}&searchtype=title&abstracts=show&order=-announced_date_first&size=50&start={}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/pdf/2010.02866.pdf', 'https://arxiv.org/pdf/2010.02749.pdf', 'https://arxiv.org/pdf/2010.02715.pdf', 'https://arxiv.org/pdf/2010.02670.pdf', 'https://arxiv.org/pdf/2010.02576.pdf', 'https://arxiv.org/pdf/2010.02523.pdf', 'https://arxiv.org/pdf/2010.02374.pdf', 'https://arxiv.org/pdf/2010.02317.pdf', 'https://arxiv.org/pdf/2010.02213.pdf', 'https://arxiv.org/pdf/2010.02174.pdf', 'https://arxiv.org/pdf/2010.02087.pdf', 'https://arxiv.org/pdf/2010.02086.pdf', 'https://arxiv.org/pdf/2010.02011.pdf', 'https://arxiv.org/pdf/2010.01996.pdf', 'https://arxiv.org/pdf/2010.01976.pdf', 'https://arxiv.org/pdf/2010.01968.pdf', 'https://arxiv.org/pdf/2010.01711.pdf', 'https://arxiv.org/pdf/2010.01709.pdf', 'https://arxiv.org/pdf/2010.01668.pdf', 'https://arxiv.org/pdf/2010.01582.pdf', 'https://arxiv.org/pdf/2010.01431.pdf', 'https://arxiv.org/pdf/2010.01213.pdf', 'https://arxiv.org/pdf/2010.01163.pdf', 'https://arxiv.org/pdf/2010.01149.pdf', 'https://arxiv.org/pdf/2010.01030.pdf', 'https://arxiv.org/pdf/2010.00964.pdf', 'https://arxiv.org/pdf/2010.00892.pdf', 'https://arxiv.org/pdf/2010.00848.pdf', 'https://arxiv.org/pdf/2010.00826.pdf', 'https://arxiv.org/pdf/2010.00821.pdf', 'https://arxiv.org/pdf/2010.00661.pdf', 'https://arxiv.org/pdf/2010.00619.pdf', 'https://arxiv.org/pdf/2010.00536.pdf', 'https://arxiv.org/pdf/2010.00532.pdf', 'https://arxiv.org/pdf/2010.00509.pdf', 'https://arxiv.org/pdf/2010.00401.pdf', 'https://arxiv.org/pdf/2010.00399.pdf', 'https://arxiv.org/pdf/2010.00397.pdf', 'https://arxiv.org/pdf/2010.00391.pdf', 'https://arxiv.org/pdf/2010.00353.pdf', 'https://arxiv.org/pdf/2010.00350.pdf', 'https://arxiv.org/pdf/2010.00330.pdf', 'https://arxiv.org/pdf/2010.00134.pdf', 'https://arxiv.org/pdf/2010.00110.pdf', 'https://arxiv.org/pdf/2010.00072.pdf', 'https://arxiv.org/pdf/2010.00054.pdf', 'https://arxiv.org/pdf/2010.00041.pdf', 'https://arxiv.org/pdf/2009.14695.pdf', 'https://arxiv.org/pdf/2009.14623.pdf', 'https://arxiv.org/pdf/2009.14596.pdf']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# choose the query we want\n",
    "# careful: here we're just working on the first 50 results (the website presents them 50 by 50)\n",
    "# if we want more we just have to iterate on size adding 50 by 50\n",
    "\n",
    "\n",
    "            \n",
    "def get_pdf_urls(website_url, query, total_size):\n",
    "    \"\"\"\n",
    "    ARGS: \n",
    "         - total_size is the total amount of articles we have on the web page. we have to check the webpage beforehand\n",
    "        to know what it amounts to\n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for i in range (0, total_size, 50):\n",
    "        size = i\n",
    "\n",
    "        x = requests.get(website_url.format(query, size)).content\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(x, parser)\n",
    "\n",
    "        results = soup(\"p\", class_=\"list-title is-inline-block\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "                if \"pdf\" in a['href']:\n",
    "                    urls.append(a['href']+\".pdf\")\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(arxiv_url, \"machine+learning\", 50)\n",
    "print(urls)\n",
    "print(type(urls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the text\n",
    "\n",
    "Now that we have the pdf url we have two options: either use beautiful soup, see the web page as a html doc and get the texts from there; either download completely the pdf and use pdfplumber or something alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll try with beautiful soup\n",
    "\n",
    "x = requests.get(\"https://arxiv.org/pdf/2010.02866.pdf\").content\n",
    "parser = \"html.parser\"\n",
    "soup = BeautifulSoup(x, parser)\n",
    "print(soup)\n",
    "#results = soup.findall(\"span\", string = True, limit = 5)\n",
    "#print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with scrapping\n",
    "\n",
    "We'll try downloading it here first and then getting the text out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = \"/home/martin/Desktop/ImpAgt/test.pdf\"\n",
    "\n",
    "chunk_size = 4000\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2010.02866.pdf\"\n",
    "r = requests.get(url, stream=True)\n",
    "\n",
    "with open(destination, 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size):\n",
    "        fd.write(chunk)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    " - On bloque un peu dans notre scrapping sur la base de recherche du CNRS. On pense que c'est parce que le site est en dynamique donc il faut qu'on creuse un peu \n",
    " - on est parti sur un site plus simple: Arxiv\n",
    " - On arrive pas a récupérer directement les docs donc on passe par un chemin détourné: on download en bloc et on utililse un package pour ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping bigger data base: Google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.mdpi.com/327494', 'https://www.sciencedirect.com/science/article/pii/S0168169917314710', 'https://www.sciencedirect.com/science/article/pii/S0168169918304289', 'https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR019933', 'https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312', 'https://academic.oup.com/erae/article-abstract/47/3/849/5552525', 'https://www.mdpi.com/2072-4292/8/6/514', 'https://www.sciencedirect.com/science/article/pii/S0305054820300435', 'https://ieeexplore.ieee.org/abstract/document/7225403/', 'https://www.academia.edu/download/57549164/IRJET-V5I9158.pdf', 'https://www.sciencedirect.com/science/article/pii/S016816991630117X', 'https://link.springer.com/article/10.1007/s11119-014-9372-7', 'https://link.springer.com/article/10.1007/s11356-017-0496-y', 'https://www.sciencedirect.com/science/article/pii/S0168192315007467', 'https://ieeexplore.ieee.org/abstract/document/7838138/', 'https://www.sciencedirect.com/science/article/pii/S0168169917308803', 'https://www.nature.com/articles/544S21a', 'https://link.springer.com/chapter/10.1007/978-981-13-1274-8_6', 'https://www.hindawi.com/journals/complexity/2019/9190273/abs/', 'https://dl.acm.org/doi/abs/10.1145/3018896.3036385', 'https://www.sciencedirect.com/science/article/pii/S2589721719300182', 'https://ieeexplore.ieee.org/abstract/document/7325900/', 'https://www.sciencedirect.com/science/article/pii/S0168169918306987', 'https://link.springer.com/chapter/10.1007/978-3-319-94779-2_21', 'https://www.mdpi.com/2072-4292/10/8/1217', 'https://ieeexplore.ieee.org/abstract/document/7566749/', 'http://lup.lub.lu.se/student-papers/record/8971989', 'https://ieeexplore.ieee.org/abstract/document/7482764/', 'https://dl.acm.org/doi/abs/10.1145/3220228.3220242', 'http://www.academia.edu/download/58075474/AI_in_Agriculture.pdf', 'http://www.ijabe.org/index.php/ijabe/article/view/4475', 'https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87', 'https://www.sciencedirect.com/science/article/pii/S1110866520301110', 'https://www.sciencedirect.com/science/article/pii/S1360138515002630', 'https://www.sciencedirect.com/science/article/pii/S0168169917301230', 'https://www.sciencedirect.com/science/article/pii/S0048969715308822', 'https://academic.oup.com/aepp/article-abstract/40/1/79/4863692', 'https://www.sciencedirect.com/science/article/pii/S0924271618302090', 'https://www.sciencedirect.com/science/article/pii/S0168169915003671', 'https://www.sciencedirect.com/science/article/pii/S0303243415000719', 'https://ieeexplore.ieee.org/abstract/document/7473819/', 'https://www.aeaweb.org/articles?id=10.1257/aer.p20151021', 'https://www.sciencedirect.com/science/article/pii/S0048969718324021', 'https://www.sciencedirect.com/science/article/pii/S2352340919300010', 'https://www.sciencedirect.com/science/article/pii/S0016706117321031', 'https://www.sciencedirect.com/science/article/pii/S0022169418303184', 'https://www.sciencedirect.com/science/article/pii/S0924271618302260', 'https://www.aeaweb.org/articles?id=10.1257/aer.p20171038', 'https://arxiv.org/abs/1806.06762', 'https://link.springer.com/article/10.1007/s11119-017-9527-4']\n",
      "50\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n",
      "daumn\n"
     ]
    }
   ],
   "source": [
    "scholar_url = \"https://scholar.google.fr/scholar?start={}&q={}&hl=fr&as_sdt=0,5&as_ylo={}&as_yhi={}\"\n",
    "\n",
    "def get_pdf_urls(website_url, query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    ARGS: query: what we want in the research bar\n",
    "         \n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for i in range (0, 50, 10):\n",
    "        size = i\n",
    "\n",
    "        x = requests.get(website_url.format(size, query, start_date, end_date)).content\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(x, parser)\n",
    "\n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "\n",
    "                urls.append(a['href'])\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(scholar_url, \"machine+learning+agriculture\", 2015, 2020)\n",
    "print(urls)\n",
    "print(len(urls))\n",
    "for url in urls:\n",
    "    if \"sciencedirect\" in url:\n",
    "        print(\"daumn\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312\n",
      "\n",
      "Skip to Main Content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Access provided by Taylor & Francis Online\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Log in\n",
      "\n",
      " | \n",
      "\n",
      "Register\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cart\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Home\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "All Journals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, Issue 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricult ....\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search in:\n",
      "\n",
      "\n",
      "This Journal\n",
      "\n",
      "\n",
      "Anywhere\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "Advanced search\n",
      "\n",
      "Advanced search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Journal\n",
      "\n",
      "\n",
      "International Journal of Remote Sensing\n",
      "\n",
      "\n",
      "\n",
      "Volume 38, 2017 - Issue 7: European remote sensing: progress, challenges, and opportunities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "Submit an article\n",
      "Journal homepage\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "3,566\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "\n",
      "\n",
      "CrossRef citations to date\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationA. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listen\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "A. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "A. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "A. García-Pedrero Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainCorrespondenceangel.garcia@ctb.upm.esView further author information, C. Gonzalo-Martín Center for Biomedical Technology, Universidad Politécnica de Madrid, Pozuelo de Alarcón, Spain; Escuela Técnica Superior de Ingenieros Informáticos, Universidad Politécnica de Madrid, Boadilla del Monte, SpainView further author information & M. Lillo-Saavedra Faculty of Agricultural Engineering, University of Concepción, Chillán, Chile; Water Research Center for Agriculture and Mining, CRHIAM, University of Concepción, Chillán, ChileView further author information\n",
      "View further author information\n",
      "View further author information\n",
      "View further author information\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "Pages 1809-1819\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "\n",
      "Received 15 Oct 2015Accepted 23 Dec 2016Published online: 31 Jan 2017\n",
      "Received 15 Oct 2015\n",
      "Accepted 23 Dec 2016\n",
      "Published online: 31 Jan 2017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Download citation\n",
      "\n",
      "\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CrossMark\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "In this articleCloseABSTRACT1. Introduction2. Data and methods3. Results4. ConclusionsAcknowledgementsDisclosure statementAdditional informationReferences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Full Article\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figures & data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Metrics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reprints & Permissions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PDF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.Disclosure statementNo potential conflict of interest was reported by the authors.Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "ABSTRACTABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "ABSTRACT\n",
      "ABSTRACTA correct delineation of agricultural parcels is a primary requirement for any parcel-based application such as the estimate of agricultural subsidies. Currently, high-resolution remote-sensing images provide useful spatial information to delineate parcels; however, their manual processing is highly time consuming. Thus, it is necessary to create methods which allow performing this task automatically. In this work, the use of a machine-learning algorithm to delineate agricultural parcels is explored through a novel methodology. The proposed methodology combines superpixels and supervised classification in order to determine which adjacent superpixels should be merged, transforming the segmentation issue into a machine learning matter. A visual evaluation of results obtained by the methodology applied to two areas of a high-resolution satellite image of fragmented agricultural landscape points out that the use of machine-learning algorithm for this task is promising.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.\n",
      "1. IntroductionAccurate and up-to-date information about status, acreage, and the type of agricultural lands is assumed to be a valuable element for diverse agricultural-related agencies. This information allows stakeholders among other things to establish agricultural policies (Mirón Pérez 2005; van Der Molen 2002) to reduce greenhouse gas emissions, regulate water rights, and estimate subsidies. In this regard, it is important to consider that about 75% of the world’s agricultural lands are small (less than 2 ha) and family operated (Lowder, Skoet, and Raney 2016). This implies highly fragmented agricultural landscapes with a high spatial heterogeneity produced by the diversity in sizes, shapes, and crops of the different agricultural parcels. Therefore, in order to generate precise information about these agricultural lands, a primary requirement is to have a correct delineation at parcel level.The delineation of agricultural lands has been addressed with different initiatives around the world for a long time. In 1980, the National Research Council published the report Need for a Multipurpose Cadastre (NRC 1980) which has been updated by the study National Land Parcel Data: A Vision for the Future that examines the status of land-parcel data in the USA and provides a set of recommendations that would foster a national system for land parcel (NRC 2007). In the European scenario, it can be mentioned the land parcel identification systems promoted by the European Union in order to represent the activities of farmers on their lands (Leo and Lemoine 2001). These initiatives commonly use very high-resolution remotely sensed imagery to perform a manual delineation of agricultural parcel boundaries. However, a non-trivial issue is how to process a huge data volume maintaining the accuracy and time requirements. Even though the manual delineation can be very precise, it suffers from the subjectivity of operator and is highly time consuming. Moreover, the repeatability of the delineation is not insured even when the same operator performs it at two different times.To address these problems, automatic and semiautomatic methods have been proposed in the remote-sensing literature. Most of these methods are based on image segmentation. Mueller, Segl, and Kaufmann (2004) proposed an object-based approach for extracting large human-made objects, especially agricultural fields, from high-resolution imagery. This approach combined edge detection models with region-based segmentation to extract regularly shaped objects. In the work of Da Costa et al. (2007), an algorithm to automatically delineate vine parcels from very high resolution images based on their textural properties was developed. From texture attributes, they applied a thresholding method to discriminate between vine and non-vine pixels. Tiwari et al. (2009) proposed a semi-automatic methodology for extracting field boundaries from data captured by the sensor linear imaging self-scanning sensor-IV on-board ResourceSat-1 (IRS-P6) satellite. A segmentation using tonal and textural gradients was performed and the generated regions were classified to derive preliminary field boundaries. Finally, Snakes Algorithm was used to refine the geometry of these field boundaries. Turker and Kok (2013) used perceptual grouping for automatic extraction of dynamic sub-boundaries within existing agricultural fields from remote-sensing imagery. To perform field-based analysis, the approach integrated field boundary data and satellite imagery. Canny edge detector was used to detect the edge pixels. In general, approaches based on segmentation methods have the following drawbacks: (1) they are sensitive to intra-parcel variability which can produce more segments than desired, (2) most of these methods are highly dependent on a correct parameter selection (e.g. the similarity measured used to group image pixels) that requires a prior knowledge about the scene or tuning by trial error. Moreover, variability in sizes and shapes of the plots which causes a certain configuration parameters do not allow properly delineate all parcels.Recently in computer vision field, approaches intending to imitate the delineation made by an expert through supervised classification methods have been successfully applied to natural image segmentation (Nunez-Iglesias et al. 2013). Therefore, it is assumed that a similar approach could be useful for agricultural parcels delineation. The objective of this work is to establish whether approaches based on machine learning are able to correctly learn how to delineate agricultural parcels in high-resolution images.In this work, a novel methodology to delineate agricultural parcels following a supervised classification approach is presented. The proposed methodology uses superpixels as minimum processing units, whereas a process of agglomeration of superpixels is used to obtain a final segmentation where the parcels (objects of interest) are distinguished. Superpixels are a form of image segmentation; however, the focus lies more on a controlled over-segmentation. Thus, the image is divided into several homogeneous regions with a determined number of pixels. Superpixels can then be agglomerated for obtaining larger regions. In this regard, a classification method is trained using part of a segmented scene under study, to take the determination whether two adjacent superpixels should be merged. The structure of the article is the following: the data used in this study as well as the proposed methodology are described in the next Section. The obtained results are presented and discussed in Section 3. Finally, main conclusions are given in Section 4.\n",
      "2. Data and methods2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.\n",
      "2.1. Study site and datasetThe study area corresponds to a Chilean central valley (70°40′7″W, 32°48′11″S) mostly characterized by small agricultural parcels with crops of full canopy coverage and orchards. A WorldView-2 (WV-2) satellite image, acquired on 3 December 2011, was used in this study. The WV-2 image has a spatial resolution of 2.4 m and four spectral bands which properties are described in Table 1. To evaluate the proposed approach, two regions of 522 × 522 pixels each were clipped from the WV-2 scene (Figure 1). Figure 1(a,b) correspond to the areas under analysis, from here on called Image A and Image B, respectively. Agricultural parcels in both images have been manually delineated obtaining reference parcel maps. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "All authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "All authors\n",
      "A. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "https://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "Published online:31 January 2017\n",
      "Table 1.  Spectral properties of the WV-2 images.CSVDisplay Table\n",
      "Table 1.  Spectral properties of the WV-2 images.\n",
      "Table 1.  Spectral properties of the WV-2 images.\n",
      "CSVDisplay Table\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full size\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "All authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "All authors\n",
      "A. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "https://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "Published online:31 January 2017\n",
      "Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.Display full size\n",
      "Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "Display full size\n",
      "Figure 1.  Study areas are shown in a real colour composition. Borders of each parcel are displayed in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "\n",
      "2.2. MethodologyTo delineate agricultural parcels, the proposed methodology combines superpixel processing and a classification method, which provide the basis to decide when two adjacent superpixels should be merged. An overall overview of the proposed methodology is shown in Figure 2. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full sizeFigure 2.  Overall overview of the proposed methodology.The methodology starts with an over-segmentation of the image obtained through a superpixel algorithm (see Section 2.2.1). Then, from the generated superpixel representation, a dataset is created by extracting segment features of each pair of superpixels. Instances in dataset are labelled in two classes depending on if they belong to the same object (parcel) or not; this information is given by a previously generated reference parcel map. Dataset generation is described in detail in Section 2.2.2. Finally, dataset is used to generate a machine learning model, through training a classification method (Section 2.2.3). This model is later used to determine from dataset features whether two superpixels should be merged.All processes, used in this methodology, are carried out using in-house developed codes and run on the MATLAB® platform.2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 2.  Overall overview of the proposed methodology.Display full size\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "All authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "All authors\n",
      "A. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "https://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "Published online:31 January 2017\n",
      "Figure 2.  Overall overview of the proposed methodology.Display full size\n",
      "Figure 2.  Overall overview of the proposed methodology.\n",
      "Figure 2.  Overall overview of the proposed methodology.\n",
      "Display full size\n",
      "Figure 2.  Overall overview of the proposed methodology.\n",
      "\n",
      "2.2.1. Superpixel processingA superpixel is a small, local, and coherent cluster which contains a statistically homogeneous image region according to certain criteria such as colour, texture, among others (Ren and Malik 2003). Superpixels are a form of image segmentation, but the focus lies more on an image over-segmentation, not on segmenting meaningful objects (Schick and Stiefelhagen 2011). In this regard, superpixel processing is not seen as an end in itself but rather a preprocessing step in order to solve a major problem, in this case the efficient analysis of a scene. Superpixel techniques enhance image analysis, e.g. reducing the influence of noise and intra-class spectral variability, preserving most edges of images, and improving the computational speed of later steps such as the segmentation of meaningful objects (Achanta et al. 2012).Superpixel processing is carried out by a modified version of the segmentation method called simple linear iterative clustering (SLIC) (Achanta et al. 2012), which is in turn based on the well-known k-means method, to group image pixels into superpixels. The original SLIC algorithm works in the RGB colour space (defined by only the Red, Green, and Blue spectral bands) and considers two parameters: k, the desired number of superpixels, and c, the compactness factor. A larger value of c emphasizes the importance of the spatial proximity resulting in more compact superpixels. The SLIC version used in this work corresponds to the implemented by Gonzalo Martín et al. (2015), which extends the method to work with multispectral images.\n",
      "2.2.2. Creation of datasetAll available spectral bands (i.e. four bands, from blue to near infrared) as well as three spectral indices commonly used in remote-sensing image analysis are used for feature extraction. The spectral indices (defined in Equation (1)–(3)) used in this study are normalized difference vegetation index (Rouse et al. 1974), normalized difference water index (Gao 1996), and spectral shape index (Chen et al. 2009). (1) (2) (3) where , , , and  represent the red, blue, green, and near-infrared spectral bands, respectively. In addition, a set of texture-based features is computed using local entropy (Equation (4)) from the above features, varying the size of the neighbourhood (N) in which entropy is measured. The local entropy is calculated as follows (Gonzalez, Woods, and Eddins 2004) (4) where  is a random variable indicating intensity,  is the histogram of the intensity levels in N, and l is the number of possible intensity levels.Thus, using aforementioned features (e.g. spectral indices, and texture), each superpixel is characterized by a feature vector (f), whose components are defined by the average value of the pixel feature values that are part of it. Finally, the dataset F, used in the classification process, is created by characterizing each pair of superpixels (i and j), using their corresponding feature vectors, as follows (5) where  is the feature vector of the  superpixel, and  represents the absolute value. The main idea is to create a multidimensional feature space, in which a machine-learning (classifier) method learns a function able to predict when a pair of superpixels belong to the same object. In this work, an absolute difference is used; however, pairs of superpixels can be characterized in diverse ways (e.g. by applying different distance measures or by concatenating both feature vectors); in this regard, the feature vector representing the relationship between those superpixels must be a vector with some properties such as symmetry (i.e. ), and it cannot be negative.Since the aim of this work is to agglomerate superpixels, only those that are adjacent are considered. A label set L (target labels) is created using the information about the adjacency of superpixels and the available reference parcel map (ground-truth data). Thus, each instance of dataset F is labelled according to Equation (6). (6) where i corresponds to the  superpixel and  represents the label of the instance . Here, a positive label for  indicates that superpixels i and j should be merged, whereas a negative one means that both superpixels belong to different objects; hence, they should not be merged.\n",
      "2.2.3. Classification processDue to the characteristics of the landscape under analysis, target labels are imbalanced (i.e. there are more positive labels than negative, or in other words, more pairs of superpixels belong to the same object). Therefore, a classifier that considers this distribution of the classes is needed in order to obtain satisfactory results. For this reason, the RUSBoost algorithm is used as classifier.RUSBoost is a hybrid boosting/sampling method proposed by Seiffert et al. (2010), which is a state-of-the-art method for learning from imbalanced datasets. RUSBoost improves boosting algorithm by resampling training data in order to balance the class distribution. Unlike other ensemble methods, RUSBoost applies an under-sampling strategy to randomly remove samples from the majority class, before the training of each weak learner algorithm that is part of the ensemble. It combines many weak classifiers  into a strong classifier G by linear combination. The final classifier is constructed as (7) where  represents the output of classifier , expressed as the posterior probability for the class  given the feature vector x.The resulting class y of RUSBoost method, given the input feature vector x, is the one that gets the maximum value. The weak learners are added incrementally to . In each iteration t, RUSBoost randomly subsamples the majority class in training set X until a subset  with a desired class distribution is reached. For example, if the desired class ratio r is 50:50, then the majority class examples are randomly removed until the numbers of majority and minority class examples are equal. Hence, a weight  is assigned to the weak learner according to the relation (8) where  represents the pseudo loss based on the original training set X and it is calculated as (9) where  expresses the posterior probability of the classifier  for a class  that is different from the real class. D is a weight distribution for all examples in X, which weights () are updated after each iteration as follows (10) and then,  is normalized to 1. Initially, the weight of each example  is set to , where n is the number of examples in the training set (X).During classification process, dataset F and label set L provide the feature vectors and their corresponding classes in the form of the patterns  that the classifier must learn in order to delineate agricultural parcels.\n",
      "3. ResultsTo prove the potential of the proposed approach, two experiments have been carried out. The first one uses the image A to generate the classification model which is subsequently tested on image B. The second experiment is similar to previous one but interchanging the images used to generate and test the model.From each image, a total of 5450 superpixels were automatically generated through modified SLIC method (i.e. extended to multispectral images), where each superpixel is composed of 50 pixels on average. The number of pixels that constitutes each superpixel was chosen experimentally to agree with the size of most of the treetops present in the images under analysis. They represent 2% of observations to analyse respect to the entire number of pixels under analysis per image. Due to visualization issues, only enlarged regions containing a set of generated superpixels are shown in Figure 3. As can be observed, superpixels adhere well to the boundaries of spectrally homogeneous regions, in particular, borders of parcels are well delineated. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full sizeFigure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.The number of instances generated for scenes A and B is 12,691 (77.44% positive and 22.56% negative) and 12,994 (78.75% positive and 21.24% negative), respectively. Each instance was characterized by a vector of 28 features: seven corresponding to the four bands and the spectral indexes, and the remaining 21 to the local entropy computed on neighbourhoods of three different sizes (9, 17, 33) over the seven first features. These instances were used to create two datasets, one for each image. During the classification step, a total of 1000 decision trees were used as weak classifiers to build a single RUSBoost classifier using a ratio of sampling of 50:50.To evaluate the results obtained by the classifier, a 10-fold cross validation was performed separately using both datasets. During each fold, non-overlapping testing and validation sets (which correspond to seen data) were generated by randomly selecting positive and negative instances maintaining their original class distribution. The obtained results showed mean accuracies (user and producer) greater than 89% with a small standard deviation (lower than 1.09 on average) for positive instances (merge), whereas user and producer accuracies lie between 73% and 78% in the case of negative instances. This may occur because there are few negative (do not merge) instances for training the classifier, requiring more negative instances to improve these results. The assessment of classification using the test set is displayed in Table 2. The low variability of the accuracies in 10 folds points out that the method is stable; therefore, similar results are expected during the validation process. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table To test our approach, the classifier of the fold with the best overall accuracy in the validation set was used to determine which superpixels should be merged in the test image. Thus, overall accuracies of 83.53% and 85.58% were obtained using as input the test images (unseen data) A and B, respectively. The results of applying these models to validation and test data are shown in Figure 4, as seen the results in validation set fit better with the ground truth than the results obtained in test set. However, in both cases, most of the superpixels were correctly merged, indicating that better results can be obtained by improving the methodology. A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full sizeFigure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full size\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "All authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "All authors\n",
      "A. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "https://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "Published online:31 January 2017\n",
      "Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.Display full size\n",
      "Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "Display full size\n",
      "Figure 3.  Superpixel segmentation of two small areas. Borders of each superpixel are shown in red. (a) and (b) Correspond to Images A and B, respectively.\n",
      "\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "All authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "All authors\n",
      "A. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "https://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "Published online:31 January 2017\n",
      "Table 2.  Accuracy assessment of the classification (validation set).CSVDisplay Table\n",
      "Table 2.  Accuracy assessment of the classification (validation set).\n",
      "Table 2.  Accuracy assessment of the classification (validation set).\n",
      "CSVDisplay Table\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full size\n",
      "A machine learning approach for agricultural parcel delineation through agglomerative segmentationAll authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedrahttps://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "All authorsA. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "All authors\n",
      "A. García-Pedrero, C. Gonzalo-Martín & M. Lillo-Saavedra\n",
      "https://doi.org/10.1080/01431161.2016.1278312Published online:31 January 2017\n",
      "https://doi.org/10.1080/01431161.2016.1278312\n",
      "Published online:31 January 2017\n",
      "Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.Display full size\n",
      "Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.\n",
      "Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.\n",
      "Display full size\n",
      "Figure 4.  First row shows the results obtained by the best models in the seen data for Images A (a) and B (b), as well as the results of the same model applied to unseen data (second row) for Images A (c) and B (d). For visualization purposes, only it is shown that white borders of superpixels should be separated.\n",
      "\n",
      "4. ConclusionsThis article has presented a methodology for the automatic delineation of agricultural parcels in high-resolution images (WV-2). The proposed methodology uses an extended version of SLIC algorithm for over-segmentating the image to generate superpixels and a supervised classification method to determine when adjacent superpixels should be merged. The results showed that it is possible to train a machine learning to delineate agricultural parcels. In this regard, learning from data, how agricultural parcel is delineated poses an alternative to traditional segmentation algorithms, which could be exploited to imitate the labour of a human operator. Two main aspects will be improved in future research: (1) determining the optimal features to train the methodology, (2) exploring different ways to measure the similarity of adjacent superpixels (e.g. testing diverse distances), and (3) extending the process to different interest objects and scales.\n",
      "Table 1.  Spectral properties of the WV-2 images.BandSpectral range (nm)Blue450–510Green510–580Red630–690Near infrared770–895\n",
      "Table 1.  Spectral properties of the WV-2 images.\n",
      "Table 2.  Accuracy assessment of the classification (validation set).ImageClassUser’s accuracy (%)Producer’s accuracy (%)A+ (merge)94.01 ± 1.0992.85 ± 0.84− (do not merge)75.13 ± 3.3278.63 ± 2.85B+ (merge)89.58 ± 0.8792.97 ± 0.71− (do not merge)73.66 ± 2.9576.57 ± 2.99\n",
      "Table 2.  Accuracy assessment of the classification (validation set).\n",
      "AcknowledgementsA. García-Pedrero (Grant Number 216146) acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT). This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07). The authors would like to thank the Editors and the anonymous reviewers for their helpful and constructive comments that greatly contributed to improving this manuscript. They would also like to thank Dr. Rodolfo Guzman Huerta for his comments and suggestions for improving this manuscript.\n",
      "Disclosure statementNo potential conflict of interest was reported by the authors.\n",
      "Disclosure statementNo potential conflict of interest was reported by the authors.\n",
      "Additional informationFundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "FundingA. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "A. García-Pedrero acknowledges the support for the realization of his doctoral thesis to the Mexican National Council of Science and Technology (CONACyT) [Grant Number 216146]. This work has been funded by the Fondo de Fomento al Desarrollo Científico y Tecnológico (FONDEF IT13I20002) through the project entitled ‘AQUASAT: Un servicio integrado para el manejo sitio-específico del agua de riego’ and by the Centro de Recursos Hídricos para la Agricultura y la Minería (CONICYT/FONDAP/1513001) and by the Universidad Politécnica de Madrid (AL-16-PID-07)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReferencesAchanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk. 2012. “SLIC Superpixels Compared to State-Of-The-Art Superpixel Methods.” IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (11): 2274–2282. doi:10.1109/TPAMI.2012.120. [Crossref], [PubMed], [Web of Science ®], [Google Scholar]Chen, Y., W. Su, J. Li, and Z. Sun. 2009. “Hierarchical Object Oriented Classification Using Very High Resolution Imagery And Lidar Data Over Urban Areas.” Advances In Space Research 43 (7): 1101–1110. doi:10.1016/j.asr.2008.11.008. [Crossref], [Web of Science ®], [Google Scholar]Da Costa, J. P., F. Michelet, C. Germain, O. Lavialle, and G. Grenier. 2007. “Delineation of Vine Parcels by Segmentation of High Resolution Remote Sensed Images.” Precision Agriculture 8 (1–2): 95–110. doi:10.1007/s11119-007-9031-3. [Crossref], [Web of Science ®], [Google Scholar]Gao, B.-C. 1996. “NDWI—A Normalized Difference Water Index for Remote Sensing of Vegetation Liquid Water from Space.” Remote Sensing of Environment 58 (3): 257–266. doi:10.1016/S0034-4257(96)00067-3. [Crossref], [Web of Science ®], [Google Scholar]Gonzalez, R. C., R. E. Woods, and S. L. Eddins. 2004. Digital Image Processing Using MATLAB. Upper Saddle River, New Jersey: Pearson Education India. [Google Scholar]Gonzalo Martín, C., E. Menasalvas, M. Lillo Saavedra, D. Fonseca-Luengo, A. Garcia-Pedrero, and R. Costumero. 2015. “Local Optimal Scale in a Hierarchical Segmentation Method for Satellite Images.” Journal of Intelligent Information Systems 46(3): 517-529. [PubMed], [Web of Science ®], [Google Scholar]Leo, O., and G. Lemoine. 2001. “Land Parcel Identification Systems in the Frame of Regulaton (EC) 1593/2000 Version 1.4.” Tech. rep. Ispra, Italia: Institute for Environment and Sustainability. [Google Scholar]Lowder, S. K., J. Skoet, and T. Raney. 2016. “The Number, Size, and Distribution of Farms, Smallholder Farms, and Family Farms Worldwide.” World Development. 87: 16-29. [Web of Science ®], [Google Scholar]Mirón Pérez, J. 2005. “Cadastre and the Reform of European Union's Common Agricultural Policy. Implementation of the SIGPAC.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct54/01-catastro_54_ing.pdf [Google Scholar]Mueller, M., K. Segl, and H. Kaufmann. 2004. “Edge-and Region-Based Segmentation Technique for the Extraction of Large, Man-Made Objects in High-Resolution Satellite Imagery.” Pattern Recognition 37 (8): 1619–1628. doi:10.1016/j.patcog.2004.03.001. [Crossref], [Web of Science ®], [Google Scholar]National Research Council. 1980. Need for a Multipurpose Cadastre. Washington, DC: National Academy Press. [Google Scholar]National Research Council. 2007. National Land Parcel Data: A Vision for the Future. Washington, DC: The National Academies Press. [Google Scholar]Nunez-Iglesias, J., R. Kennedy, T. Parag, J. Shi, and D. B. Chklovskii. 2013. “Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images.” PLoS One 8: 1–11. doi:10.1371/journal.pone.0071715. [Crossref], [Web of Science ®], [Google Scholar]Ren, X., and J. Malik. 2003. “Learning a Classification Model for Segmentation.” In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, October10–17, edited by Bob Werner, Vol. 1. Los Alamitos, California: IEEE. [Google Scholar]Rouse, J. _. W. Jr, R. H. Haas, J. A. Schell, and D. W. Deering. 1974. “Monitoring Vegetation Systems in the Great Plains with ERTS.” NASA Special Publication 351: 309. [Google Scholar]Schick, A., and R. Stiefelhagen. 2011. “Evaluating Image Segments by Applying the Description Length to Sets of Superpixels.” In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 1394–1401. IEEE. [Google Scholar]Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse, and A. Napolitano. 2010. “Rusboost: A Hybrid Approach to Alleviating Class Imbalance.” IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (1): 185–197. doi:10.1109/TSMCA.2009.2029559. [Crossref], [Web of Science ®], [Google Scholar]Tiwari, P. S., H. Pande, M. Kumar, and V. K. Dadhwal. 2009. “Potential of IRS P-6 LISS IV for Agriculture Field Boundary Delineation.” Journal of Applied Remote Sensing 3 (1): 033528–033528. doi:10.1117/1.3133306. [Crossref], [Web of Science ®], [Google Scholar]Turker, M., and E. H. Kok. 2013. “Field-Based Sub-Boundary Extraction from Remote Sensing Imagery Using Perceptual Grouping.” ISPRS Journal of Photogrammetry and Remote Sensing 79: 106–121. doi:10.1016/j.isprsjprs.2013.02.009. [Crossref], [Web of Science ®], [Google Scholar]van Der Molen, P. 2002. “The use of the Cadastre among the Members States: Property rights, land registration and Cadastre in the European Union.” http://www.catastro.meh.es/documentos/publicaciones/ct/ct45/02ingles.pdf [Google Scholar]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Metrics\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "Views\n",
      "\n",
      "\n",
      "3566\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "Crossref\n",
      "19\n",
      "\n",
      "\n",
      "Web of Science\n",
      "7\n",
      "\n",
      "\n",
      "Scopus\n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Altmetric\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article metrics information\n",
      "\n",
      "Disclaimer for citing articles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "More Share Options\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Browse journals by subject\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "Back to top \n",
      "\n",
      "Back to top \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Area Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behavioral Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bioscience\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Built Environment\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Communication Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Computer Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Development Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earth Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Economics, Finance, Business & Industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Engineering & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment & Agriculture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment and Sustainability\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Food Science & Technology\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Geography\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Health and Social Care\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information Science\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language & Literature\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Law\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mathematics & Statistics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Medicine, Dentistry, Nursing & Allied Health\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Museum and Heritage Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Physical Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Politics & International Relations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Social Sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sports and Leisure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tourism, Hospitality and Events\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Urban Studies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "Information for\n",
      "\n",
      "Authors\n",
      "Editors\n",
      "Librarians\n",
      "Societies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "Open access\n",
      "\n",
      "Overview\n",
      "Open journals\n",
      "Open Select\n",
      "Cogent OA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "Help and info\n",
      "\n",
      "Help & contact\n",
      "Newsroom\n",
      "Commercial services\n",
      "Advertising information\n",
      "All journals\n",
      "Books\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "Keep up to date\n",
      "\n",
      "Register to receive personalised research and resources by email\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "Sign me up\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Facebook page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Twitter page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Linkedin page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Youtube page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Taylor and Francis Group Weibo page\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "Copyright © 2020 Informa UK Limited\n",
      "Privacy policy\n",
      "Cookies\n",
      "Terms & conditions\n",
      "Accessibility\n",
      "Registered in England & Wales No. 3099067\n",
      "5 Howick Place | London | SW1P 1WG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n",
      "\n",
      "Accept\n",
      "We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our Cookie Policy. By closing this message, you are consenting to our use of cookies.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_abstracts(url):\n",
    "\n",
    "    x = requests.get(url).content\n",
    "    parser = \"html.parser\"\n",
    "    soup = BeautifulSoup(x, parser)\n",
    "    #results = soup(\"div\", class_=re.compile(\"abstract\"))\n",
    "    results = soup(\"div\")\n",
    "    print(url)\n",
    "    #print(results)\n",
    "    for result in results:\n",
    "        print(result.get_text())\n",
    "\n",
    "    return 0\n",
    "\n",
    "get_abstracts(urls[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a real mess to get the abstracts: half of the websites dont let you get there (protection from robots) and the other half are so randomly coded that you get a lot of info you don't want. Most pertinent might still be to download the available pdfs and find a way to access the abstract pdfs from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
