{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Louis Ravillon, Martin Piana\n",
    "Date: October 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use this notebook:\n",
    "\n",
    "The first cells are tests we deemed interesting to keep. To succesfully scrap GoogleScholar though head directly to part3: \"Scraping scholar with chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing directly on CNRS database looks a bit complicated\n",
    "\n",
    "For now we'll just try parsing the Arxiv website and see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_url = \"https://arxiv.org/search/?query={}&searchtype=title&abstracts=show&order=-announced_date_first&size=50&start={}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/pdf/2010.02866.pdf', 'https://arxiv.org/pdf/2010.02749.pdf', 'https://arxiv.org/pdf/2010.02715.pdf', 'https://arxiv.org/pdf/2010.02670.pdf', 'https://arxiv.org/pdf/2010.02576.pdf', 'https://arxiv.org/pdf/2010.02523.pdf', 'https://arxiv.org/pdf/2010.02374.pdf', 'https://arxiv.org/pdf/2010.02317.pdf', 'https://arxiv.org/pdf/2010.02213.pdf', 'https://arxiv.org/pdf/2010.02174.pdf', 'https://arxiv.org/pdf/2010.02087.pdf', 'https://arxiv.org/pdf/2010.02086.pdf', 'https://arxiv.org/pdf/2010.02011.pdf', 'https://arxiv.org/pdf/2010.01996.pdf', 'https://arxiv.org/pdf/2010.01976.pdf', 'https://arxiv.org/pdf/2010.01968.pdf', 'https://arxiv.org/pdf/2010.01711.pdf', 'https://arxiv.org/pdf/2010.01709.pdf', 'https://arxiv.org/pdf/2010.01668.pdf', 'https://arxiv.org/pdf/2010.01582.pdf', 'https://arxiv.org/pdf/2010.01431.pdf', 'https://arxiv.org/pdf/2010.01213.pdf', 'https://arxiv.org/pdf/2010.01163.pdf', 'https://arxiv.org/pdf/2010.01149.pdf', 'https://arxiv.org/pdf/2010.01030.pdf', 'https://arxiv.org/pdf/2010.00964.pdf', 'https://arxiv.org/pdf/2010.00892.pdf', 'https://arxiv.org/pdf/2010.00848.pdf', 'https://arxiv.org/pdf/2010.00826.pdf', 'https://arxiv.org/pdf/2010.00821.pdf', 'https://arxiv.org/pdf/2010.00661.pdf', 'https://arxiv.org/pdf/2010.00619.pdf', 'https://arxiv.org/pdf/2010.00536.pdf', 'https://arxiv.org/pdf/2010.00532.pdf', 'https://arxiv.org/pdf/2010.00509.pdf', 'https://arxiv.org/pdf/2010.00401.pdf', 'https://arxiv.org/pdf/2010.00399.pdf', 'https://arxiv.org/pdf/2010.00397.pdf', 'https://arxiv.org/pdf/2010.00391.pdf', 'https://arxiv.org/pdf/2010.00353.pdf', 'https://arxiv.org/pdf/2010.00350.pdf', 'https://arxiv.org/pdf/2010.00330.pdf', 'https://arxiv.org/pdf/2010.00134.pdf', 'https://arxiv.org/pdf/2010.00110.pdf', 'https://arxiv.org/pdf/2010.00072.pdf', 'https://arxiv.org/pdf/2010.00054.pdf', 'https://arxiv.org/pdf/2010.00041.pdf', 'https://arxiv.org/pdf/2009.14695.pdf', 'https://arxiv.org/pdf/2009.14623.pdf', 'https://arxiv.org/pdf/2009.14596.pdf']\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# choose the query we want\n",
    "# careful: here we're just working on the first 50 results (the website presents them 50 by 50)\n",
    "# if we want more we just have to iterate on size adding 50 by 50\n",
    "\n",
    "\n",
    "            \n",
    "def get_pdf_urls(website_url, query, total_size):\n",
    "    \"\"\"\n",
    "    ARGS: \n",
    "         - total_size is the total amount of articles we have on the web page. we have to check the webpage beforehand\n",
    "        to know what it amounts to\n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for i in range (0, total_size, 50):\n",
    "        size = i\n",
    "\n",
    "        x = requests.get(website_url.format(query, size)).content\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(x, parser)\n",
    "\n",
    "        results = soup(\"p\", class_=\"list-title is-inline-block\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "                if \"pdf\" in a['href']:\n",
    "                    urls.append(a['href']+\".pdf\")\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(arxiv_url, \"machine+learning\", 50)\n",
    "print(urls)\n",
    "print(type(urls[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the text\n",
    "\n",
    "Now that we have the pdf url we have two options: either use beautiful soup, see the web page as a html doc and get the texts from there; either download completely the pdf and use pdfplumber or something alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We'll try with beautiful soup\n",
    "\n",
    "x = requests.get(\"https://arxiv.org/pdf/2010.02866.pdf\").content\n",
    "parser = \"html.parser\"\n",
    "soup = BeautifulSoup(x, parser)\n",
    "print(soup)\n",
    "#results = soup.findall(\"span\", string = True, limit = 5)\n",
    "#print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with scrapping\n",
    "\n",
    "We'll try downloading it here first and then getting the text out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = \"/home/martin/Desktop/ImpAgt/test.pdf\"\n",
    "\n",
    "chunk_size = 4000\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2010.02866.pdf\"\n",
    "r = requests.get(url, stream=True)\n",
    "\n",
    "with open(destination, 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size):\n",
    "        fd.write(chunk)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    " - On bloque un peu dans notre scrapping sur la base de recherche du CNRS. On pense que c'est parce que le site est en dynamique donc il faut qu'on creuse un peu \n",
    " - on est parti sur un site plus simple: Arxiv\n",
    " - On arrive pas a récupérer directement les docs donc on passe par un chemin détourné: on download en bloc et on utililse un package pour ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping bigger data base: Google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.mdpi.com/327494', 'https://www.sciencedirect.com/science/article/pii/S0168169917314710', 'https://www.sciencedirect.com/science/article/pii/S0168169918304289', 'https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR019933', 'https://academic.oup.com/erae/article-abstract/47/3/849/5552525', 'https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312', 'https://www.mdpi.com/2072-4292/8/6/514', 'https://ieeexplore.ieee.org/abstract/document/8534558/', 'https://ieeexplore.ieee.org/abstract/document/7225403/', 'https://www.sciencedirect.com/science/article/pii/S016816991630117X', 'https://link.springer.com/article/10.1007/s11119-014-9372-7', 'https://www.sciencedirect.com/science/article/pii/S0168192315007467', 'https://ieeexplore.ieee.org/abstract/document/7838138/', 'https://link.springer.com/chapter/10.1007/978-981-13-7403-6_50', 'https://www.sciencedirect.com/science/article/pii/S0168169917308803', 'https://www.nature.com/articles/544S21a', 'https://www.sciencedirect.com/science/article/pii/S2589721719300182', 'https://arxiv.org/abs/1907.10794', 'https://ieeexplore.ieee.org/abstract/document/7325900/', 'https://www.sciencedirect.com/science/article/pii/S0168169918306987', 'https://www.sciencedirect.com/science/article/pii/S0168169917308803', 'https://iopscience.iop.org/article/10.1088/1742-6596/1399/4/044109/meta', 'https://www.sciencedirect.com/science/article/pii/S0168169918306987', 'https://www.sciencedirect.com/science/article/pii/S0168169918311360', 'https://ieeexplore.ieee.org/abstract/document/7325900/', 'https://www.mdpi.com/2072-4292/10/9/1365', 'https://iopscience.iop.org/article/10.1088/1742-6596/1196/1/012065/meta', 'https://ieeexplore.ieee.org/abstract/document/7566749/', 'https://ieeexplore.ieee.org/abstract/document/7482764/', 'https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87', 'https://www.sciencedirect.com/science/article/pii/S0924271618302090', 'https://www.sciencedirect.com/science/article/pii/S0168169915003671', 'https://ieeexplore.ieee.org/abstract/document/8554406/', 'https://www.sciencedirect.com/science/article/pii/S0303243415000719', 'https://ieeexplore.ieee.org/abstract/document/8523943/', 'https://ieeexplore.ieee.org/abstract/document/7473819/', 'https://www.aeaweb.org/articles?id=10.1257/aer.p20151021', 'https://www.sciencedirect.com/science/article/pii/S0048969718324021', 'https://www.sciencedirect.com/science/article/pii/S0016706117321031', 'https://www.sciencedirect.com/science/article/pii/S0022169418303184', 'https://www.tandfonline.com/doi/abs/10.1080/15538362.2018.1485536', 'https://www.sciencedirect.com/science/article/pii/S0924271618302260', 'https://www.aeaweb.org/articles?id=10.1257/aer.p20171038', 'https://arxiv.org/abs/1806.06762', 'https://link.springer.com/article/10.1007/s11119-017-9527-4', 'https://link.springer.com/article/10.1186/s40537-017-0077-4', 'https://ieeexplore.ieee.org/abstract/document/7375220/', 'https://www.sciencedirect.com/science/article/pii/S0168169918305829', 'https://ieeexplore.ieee.org/abstract/document/8291121/', 'https://www.mdpi.com/393892']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "scholar_url = \"https://scholar.google.fr/scholar?start={}&q={}&hl=fr&as_sdt=0,5&as_ylo={}&as_yhi={}\"\n",
    "\n",
    "def get_pdf_urls(website_url, query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    ARGS: query: what we want in the research bar\n",
    "         \n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for i in range (0, 50, 10):\n",
    "        size = i\n",
    "\n",
    "        x = requests.get(website_url.format(size, query, start_date, end_date)).content\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(x, parser)\n",
    "\n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "\n",
    "                urls.append(a['href'])\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(scholar_url, \"machine+learning+agriculture\", 2015, 2020)\n",
    "print(urls)\n",
    "print(len(urls))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "https://www.aeaweb.org/articles?id=10.1257/aer.p20171038\n",
      "<class 'bs4.element.ResultSet'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_abstracts(url):\n",
    "\n",
    "    x = requests.get(url).content\n",
    "    parser = \"html.parser\"\n",
    "    print(type(x))\n",
    "    soup = BeautifulSoup(x, parser)\n",
    "    print(type(soup))\n",
    "    #results = soup(\"div\", class_=re.compile(\"abstract\"))\n",
    "    results = soup(\"div\")\n",
    "    print(url)\n",
    "    print(type(results))\n",
    "    for result in results:\n",
    "        print(type(result))\n",
    "        #print(result.get_text())\n",
    "\n",
    "    return 0\n",
    "\n",
    "get_abstracts(urls[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a real mess to get the abstracts: half of the websites dont let you get there (protection from robots) and the other half are so randomly coded that you get a lot of info you don't want. Most pertinent might still be to download the available pdfs and find a way to access the abstract pdfs from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14/10 : We had a call with B Frank who helped us on the bypassing of security checks we're facing with google scholar and the websites located on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nr = http.request(\\'GET\\', urls[4])\\n # transform byte information to string info\\nstring = r.data.decode(\"utf-8\")\\nprint(string)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "http = urllib3.PoolManager()\n",
    "\"\"\"\n",
    "S=0\n",
    "for i in range (len(urls)):\n",
    "    r = http.request('GET', urls[i])\n",
    "     # transform byte information to string info\n",
    "    string = r.data.decode(\"utf-8\")\n",
    "    if \"doctype html\" not in str.lower(string):\n",
    "        print(str.lower(string[:15]))\n",
    "        print(i)\n",
    "        S+=1\n",
    "print(S)\n",
    "\"\"\"\n",
    "\n",
    "#it seems that using this method we get access to the html whereas previously we didn't\n",
    "# we still have one or 2 problems(apparentlu with captchas) but it represents a small percentage \n",
    "# of the articles apparently\n",
    "# you can check by uncommenting the lines below\n",
    "\"\"\"\n",
    "r = http.request('GET', urls[4])\n",
    " # transform byte information to string info\n",
    "string = r.data.decode(\"utf-8\")\n",
    "print(string)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://academic.oup.com/erae/article-abstract/47/3/849/5552525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor result in results:\\n    print(result.get_text())\\n\\n        results = soup(\"h3\", class_=\"gs_rt\")\\n\\n\\n        for result in results:\\n            for a in result.find_all(\\'a\\', href=True):\\n\\n                urls.append(a[\\'href\\'])\\n    return urls\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By running this cell and uncommenting \"print soup\" you'll realize that for some \n",
    "# reason the abstract isn't present in the html\n",
    "\n",
    "r = http.request('GET', urls[4])\n",
    "print(urls[4])\n",
    " # transform byte information to string info\n",
    "    \n",
    "string = r.data.decode(\"utf-8\")\n",
    "\n",
    "#print(string)\n",
    "byte_page = r.data\n",
    "\n",
    "\n",
    "\n",
    "parser = \"html.parser\"\n",
    "soup = BeautifulSoup(byte_page, parser)\n",
    "#print(soup)\n",
    "results = soup(\"div\", class_=\"abstractSection abstractInFull\")\n",
    "#print(results)\n",
    "\"\"\"\n",
    "for result in results:\n",
    "    print(result.get_text())\n",
    "\n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "\n",
    "                urls.append(a['href'])\n",
    "    return urls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scraping scholar with geckodriver (safari)\n",
    "\n",
    "On this example we have a pb: we are getting missing parts in the html. This is due to he dynamic website? We might need to use selenium\n",
    "\n",
    "Installing Selenium with the right version of geckodriver etc was a mess. I recommend following these steps for it to work: https://tecadmin.net/setup-selenium-with-firefox-on-ubuntu/\n",
    "(go to step 4 included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: use setter for headless property instead of set_headless\n",
      "  \n",
      "/home/martin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: use options instead of firefox_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "\n",
    "# Note: You'll have to adapt your paths here. I installed my geckodriver on my desktop. Youll have to find where it is\n",
    "# to find where your firefox is on linux: \"which firefox\" in terminal\n",
    "\n",
    "\n",
    "binary = FirefoxBinary('/usr/bin/firefox')\n",
    "binary = r'/usr/bin/firefox'\n",
    "options = Options()\n",
    "options.set_headless(headless=True)\n",
    "options.binary = binary\n",
    "\n",
    "cap = DesiredCapabilities().FIREFOX\n",
    "cap[\"marionette\"] = True #optional\n",
    "browser = webdriver.Firefox(firefox_options=options, capabilities=cap,executable_path='/home/martin/Desktop/geckodriver-v0.25.0-linux64/geckodriver')\n",
    "\n",
    "# get source code\n",
    "browser.get('https://www.sciencedirect.com/science/article/pii/S0168169917314710')\n",
    "html = browser.page_source\n",
    "\n",
    "print(type(html))\n",
    "#print(html)\n",
    "# close web browser\n",
    "browser.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new method every thing seems to be working: the abstract at least is present as you can see if you uncomment the \"print (html)\" in the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: use setter for headless property instead of set_headless\n",
      "/home/martin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:40: DeprecationWarning: use options instead of firefox_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useragent:  Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:25.0) Gecko/20100101 Firefox/25.0\n",
      "['https://www.mdpi.com/327494', 'https://www.sciencedirect.com/science/article/pii/S0168169917314710', 'https://www.sciencedirect.com/science/article/pii/S0168169918304289', 'https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR019933', 'https://academic.oup.com/erae/article-abstract/47/3/849/5552525', 'https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312', 'https://www.mdpi.com/2072-4292/8/6/514', 'https://ieeexplore.ieee.org/abstract/document/8534558/', 'https://ieeexplore.ieee.org/abstract/document/7225403/', 'https://www.sciencedirect.com/science/article/pii/S016816991630117X']\n",
      "10\n",
      "non\n"
     ]
    }
   ],
   "source": [
    "scholar_url = \"https://scholar.google.se/scholar?start={}&q={}&hl=fr&as_sdt=0,5&as_ylo={}&as_yhi={}\"\n",
    "\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "def get_pdf_urls(website_url, query, start_date, end_date):\n",
    "    \"\"\"\n",
    "    ARGS: query: what we want in the research bar\n",
    "         \n",
    "    OUTPUT: a list of urls linking to the articles\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #https://stackoverflow.com/questions/58873022/how-to-make-selenium-script-undetectable-using-geckodriver-and-firefox-through-p\n",
    "    profile = webdriver.FirefoxProfile('/home/martin/.mozilla/firefox/9ncorkym.ImpAgt-user')\n",
    "\n",
    "    PROXY_HOST = \"12.12.12.123\"\n",
    "    PROXY_PORT = \"1234\"\n",
    "    profile.set_preference(\"network.proxy.type\", 1)\n",
    "    profile.set_preference(\"network.proxy.http\", PROXY_HOST)\n",
    "    profile.set_preference(\"network.proxy.http_port\", int(PROXY_PORT))\n",
    "    profile.set_preference(\"dom.webdriver.enabled\", False)\n",
    "    profile.set_preference('useAutomationExtension', False)\n",
    "    profile.update_preferences()\n",
    "    \n",
    "    \n",
    "    \n",
    "    binary = FirefoxBinary('/usr/bin/firefox')\n",
    "    binary = r'/usr/bin/firefox'\n",
    "    options = Options()\n",
    "    options.set_headless(headless=True)\n",
    "    options.binary = binary\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    print(\"useragent: \", userAgent)\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "\n",
    "    cap = DesiredCapabilities().FIREFOX\n",
    "    cap[\"marionette\"] = True #optional\n",
    "    browser = webdriver.Firefox(firefox_profile=profile, firefox_options=options, capabilities=cap,executable_path='/home/martin/Desktop/geckodriver-v0.25.0-linux64/geckodriver')\n",
    "    \n",
    "    \n",
    "    \n",
    "    urls = []\n",
    "    for i in range (0, 10, 10):\n",
    "        size = i\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        binary = r'/usr/bin/firefox'\n",
    "        options = Options()\n",
    "        options.set_headless(headless=True)\n",
    "        options.binary = binary\n",
    "        ua = UserAgent()\n",
    "        userAgent = ua.random\n",
    "        print(\"useragent: \", userAgent)\n",
    "        options.add_argument(f'user-agent={userAgent}')\n",
    "\n",
    "        cap = DesiredCapabilities().FIREFOX\n",
    "        cap[\"marionette\"] = True #optional\n",
    "        browser = webdriver.Firefox(firefox_options=options, capabilities=cap,executable_path='/home/martin/Desktop/geckodriver-v0.25.0-linux64/geckodriver')\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        browser.get(website_url.format(size, query, start_date, end_date))\n",
    "        time.sleep(1)\n",
    "        scholar_html = (browser.page_source)\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(scholar_html, parser)\n",
    "        \n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "\n",
    "                urls.append(a['href'])\n",
    "    browser.close()\n",
    "    return urls\n",
    "\n",
    "urls = get_pdf_urls(scholar_url, \"machine+learning+agriculture\", 2015, 2020)\n",
    "print(urls)\n",
    "print(len(urls))\n",
    "\n",
    "print(\"non\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sraping Scholar with chromedriver\n",
    "\n",
    "\n",
    "Installing chrome and chromedriver on ubuntu: https://christopher.su/2015/selenium-chromedriver-ubuntu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "\n",
    "# free proxies: https://spys.one/en/https-ssl-proxy/\n",
    "# https://free-proxy-list.net/\n",
    "\n",
    "scholar_url = \"https://scholar.google.se/scholar?start={}&q={}&hl=fr&as_sdt=0,5&as_ylo={}&as_yhi={}\"\n",
    "\n",
    "\n",
    "# Load driver (for Google Chrome)  \n",
    "chromedriver = \"/usr/bin/chromedriver\" # chromedriver is the connection between our python code and the browser\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "def get_urls(website_url, query, start_date, end_date):\n",
    "    urls = []\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    for i in range (0, 1000, 10):\n",
    "        \n",
    "        options = Options()\n",
    "        \n",
    "        options.add_arguments(\"--kiosk\") # for the viewport\n",
    "        \n",
    "        ua = UserAgent()\n",
    "        PROXY = \"http://161.97.84.211:3128\"\n",
    "\n",
    "        \n",
    "        proxy = {'address': PROXY,\n",
    "                 'username': 'USERNAME',\n",
    "                 'password': 'PASSWORD'}\n",
    "\n",
    "        capabilities = dict()\n",
    "        capabilities['proxy'] = {'proxyType': 'MANUAL',\n",
    "                                 'httpProxy': proxy['address'],\n",
    "                                 'ftpProxy': proxy['address'],\n",
    "                                 'sslProxy': proxy['address'],\n",
    "                                 'noProxy': '',\n",
    "                                 'class': \"org.openqa.selenium.Proxy\",\n",
    "                                 'autodetect': False}\n",
    "        \n",
    "        \n",
    "        \n",
    "        #options.add_argument('--proxy-server={}'.format(PROXY))\n",
    "        userAgent = ua.random\n",
    "        print(userAgent)\n",
    "        options.add_argument(f'user-agent={userAgent}')\n",
    "        driver = webdriver.Chrome(chromedriver,  desired_capabilities=capabilities)#chrome_options=options,\n",
    "        driver.get(website_url.format(i, query, start_date, end_date))\n",
    "        driver.implicitly_wait(55)\n",
    "        time.sleep(1)\n",
    "        scholar_html = driver.page_source\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(scholar_html, parser)\n",
    "        \n",
    "        results = soup(\"h3\", class_=\"gs_rt\")\n",
    "\n",
    "        for result in results:\n",
    "            for a in result.find_all('a', href=True):\n",
    "                \n",
    "                urls.append(a['href'])\n",
    "      \n",
    "    driver.quit() # closing the webdriver \n",
    "    return urls\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Options' object has no attribute 'addArguments'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ce4325c1283a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"agriculture\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscholar_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"crops\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2015\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b6b4fe1484ae>\u001b[0m in \u001b[0;36mget_urls\u001b[0;34m(website_url, query, start_date, end_date)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--kiosk\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for the viewport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mua\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Options' object has no attribute 'addArguments'"
     ]
    }
   ],
   "source": [
    "# getting the major article sources\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def count(urls):\n",
    "    short_urls = []\n",
    "    for i in range (len(urls)):\n",
    "        short_urls.append(urls[i][:urls[i].find(\"/\", 8, -1)])\n",
    "\n",
    "    counted = Counter(short_urls)\n",
    "    return counted\n",
    "\n",
    "\n",
    "\n",
    "urls = get_urls(scholar_url, \"agriculture\", 2015, 2020)\n",
    "print(count(urls)[:20])\n",
    "urls = get_urls(scholar_url, \"crops\", 2015, 2020)\n",
    "print(count(urls)[20])\n",
    "urls = get_urls(scholar_url, \"machine+learning\", 2015, 2020)\n",
    "print(count(urls)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.mdpi.com/327494', 'https://www.sciencedirect.com/science/article/pii/S0168169917314710', 'https://www.sciencedirect.com/science/article/pii/S0168169918304289', 'https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR019933', 'https://academic.oup.com/erae/article-abstract/47/3/849/5552525', 'https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312', 'https://link.springer.com/article/10.1007/s11119-014-9372-7', 'https://www.mdpi.com/2072-4292/8/6/514', 'https://www.sciencedirect.com/science/article/pii/S0168192315007467', 'https://www.sciencedirect.com/science/article/pii/S0168169918306987']\n",
      "html extracted from https://www.mdpi.com/327494\n",
      "html extracted from https://www.sciencedirect.com/science/article/pii/S0168169917314710\n",
      "html extracted from https://www.sciencedirect.com/science/article/pii/S0168169918304289\n",
      "html extracted from https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2016WR019933\n",
      "html extracted from https://academic.oup.com/erae/article-abstract/47/3/849/5552525\n",
      "html extracted from https://www.tandfonline.com/doi/abs/10.1080/01431161.2016.1278312\n",
      "html extracted from https://link.springer.com/article/10.1007/s11119-014-9372-7\n",
      "html extracted from https://www.mdpi.com/2072-4292/8/6/514\n",
      "html extracted from https://www.sciencedirect.com/science/article/pii/S0168192315007467\n",
      "html extracted from https://www.sciencedirect.com/science/article/pii/S0168169918306987\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ni=0\\nfor html in htmls:\\n    text=\\'\\'\\n    parser = \"html.parser\"\\n    soup = BeautifulSoup(html, parser)\\n    results = soup(\"div\")\\n    print(\"\\n\")\\n    print(\"********\")\\n    print(\"\\n\")\\n    for result in results:\\n        a=result.get_text()\\n        if len(a)>1000 and len(a)<4000 and \"\\n\\n\" not in a and \"ScienceDirectJournals\" not in a and \"AccessGet\" not in a :\\n            text+=\\'\\n\\'+a\\n            #print(a)\\n            #print(\"iiiiiiiiiiiiiiiii\")\\n    if len(text)>0:\\n        print(\"this ones good: \", i)\\n        print(text)\\n    else:\\n        print(i)\\n\\n    i+=1\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# get source code\n",
    "def get_htmls():\n",
    "    htmls = []\n",
    "    print(urls)\n",
    "    for url in urls:\n",
    "\n",
    "        try:\n",
    "            driver = webdriver.Chrome(chromedriver)\n",
    "            driver.get(url)\n",
    "            htmls.append(driver.page_source)\n",
    "            print(\"html extracted from\", url)\n",
    "            driver.quit()\n",
    "        except:\n",
    "            print(\"url {} didnt work\".format(url))\n",
    "    return htmls\n",
    "#print(htmls[0])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts(htmls):\n",
    "    i=0\n",
    "    for html in htmls:\n",
    "        text=''\n",
    "        parser = \"html.parser\"\n",
    "        soup = BeautifulSoup(html, parser)\n",
    "        results = soup(\"div\")\n",
    "        print(\"\\n\")\n",
    "        print(\"********\")\n",
    "        print(\"\\n\")\n",
    "        for result in results:\n",
    "            a=result.get_text()\n",
    "            if len(a)>1000 and len(a)<4000 and \"\\n\\n\" not in a and \"ScienceDirectJournals\" not in a and \"AccessGet\" not in a :\n",
    "                text+='\\n'+a\n",
    "                #print(a)\n",
    "                #print(\"iiiiiiiiiiiiiiiii\")\n",
    "        if len(text)>0:\n",
    "            print(\"this ones good: \", i)\n",
    "            print(text)\n",
    "        else:\n",
    "            print(i)\n",
    "\n",
    "        i+=1\n",
    "    return 0\n",
    "\n",
    "get_abstracts(htmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This abstract extraction is very noisy: due to the differences in the way the websites are coded it is difficult to find an exact bs4 command retrieving all abstracts.\n",
    "We are considering training a model capable of detecting where the abstract starts and begins in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
