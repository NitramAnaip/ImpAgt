{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore PubAG API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"jGgEv0rYItqngfgOd9eF9HN5GoCDRxTzB64slFdL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938\n"
     ]
    }
   ],
   "source": [
    "with open('pubag_abs_dict.json', 'r') as read_file:\n",
    "    abstract_dict = json.load(read_file)\n",
    "print(len(abstract_dict['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 23 abstracts\n"
     ]
    }
   ],
   "source": [
    "with open('pubag_abs_dict.json', 'r') as read_file:\n",
    "    abstract_dict = json.load(read_file)\n",
    "\n",
    "start_len = len(abstract_dict['id'])\n",
    "keywords = \"convolutional+neural+networks+wheat\"\n",
    "#put convolutional neural network. CNN stands for Cheyenne or aminés (chemistry) etc\n",
    "response = requests.get(\"https://api.nal.usda.gov/pubag/rest/search/?query={}&per_page=100&api_key={}\".format(keywords, API_KEY)).json()\n",
    "totalpages = response[\"request\"][\"totalPages\"]\n",
    "\n",
    "for i in range (totalpages):\n",
    "    response = requests.get(\"https://api.nal.usda.gov/pubag/rest/search/?query={}&page={}&per_page=100&api_key={}\".format(keywords, i+1, API_KEY)).json()\n",
    "    for j in range (len(response['resultList'])):\n",
    "        abs_id = response['resultList'][j]['id']\n",
    "        if abs_id not in abstract_dict['id']:\n",
    "            try:\n",
    "                abstract_dict['id'].append(abs_id)\n",
    "                abstract_dict['titles'].append(response['resultList'][j]['title'])\n",
    "                abstract_dict['abstracts'].append(response['resultList'][j]['abstract'])\n",
    "                abstract_dict['authors'].append((response['resultList'][j]['author']))\n",
    "                abstract_dict['keywords'].append((keywords))\n",
    "                abstract_dict['subject'].append((response['resultList'][j]['subject']))\n",
    "                abstract_dict['date'].append((response['resultList'][j]['date']))\n",
    "                abstract_dict['sources'].append((response['resultList'][j]['journal']))\n",
    "            except:\n",
    "                print(abs_id, \" didn't work\")\n",
    "end_len = len(abstract_dict['id'])\n",
    "\n",
    "print(\"added {} abstracts\".format(end_len-start_len))\n",
    "\n",
    "\n",
    "with open('pubag_abs_dict.json', 'w+') as f:\n",
    "    json.dump(abstract_dict, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_dict = {'id': [], 'titles': [], 'abstracts': [], 'authors': [], 'keywords': [], 'subject': [], 'date': [], 'sources':[]}\n",
    "with open('pubag_abs_dict.json', 'w+') as f:\n",
    "    json.dump(abs_dict, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improving the accuracy of edge pixel classification is crucial for extracting the winter wheat spatial distribution from remote sensing imagery using convolutional neural networks (CNNs). In this study, we proposed an approach using a partly connected conditional random field model (PCCRF) to refine the classification results of RefineNet, named RefineNet-PCCRF. First, we used an improved RefineNet model to initially segment remote sensing images, followed by obtaining the category probability vectors for each pixel and initial pixel-by-pixel classification result. Second, using manual labels as references, we performed a statistical analysis on the results to select pixels that required optimization. Third, based on prior knowledge, we redefined the pairwise potential energy, used a linear model to connect different levels of potential energies, and used only pixel pairs associated with the selected pixels to build the PCCRF. The trained PCCRF was then used to refine the initial pixel-by-pixel classification result. We used 37 Gaofen-2 images obtained from 2018 to 2019 of a representative Chinese winter wheat region (Tai’an City, China) to create the dataset, employed SegNet and RefineNet as the standard CNNs, and a fully connected conditional random field as the refinement methods to conduct comparison experiments. The RefineNet-PCCRF’s accuracy (94.51%), precision (92.39%), recall (90.98%), and F1-Score (91.68%) were clearly superior than the methods used for comparison. The results also show that the RefineNet-PCCRF improved the accuracy of large-scale winter wheat extraction results using remote sensing imagery.\n",
      "In the original publication of this article [1] the authors stated that important resources would be made available online to readers.\n",
      "Fungal infection represents up to 50% of yield losses, making it necessary to apply effective and cost efficient fungicide treatments, whose efficacy depends on infestation type, situation and time. In these cases, a correct and early identification of the specific infection is mandatory to minimize yield losses and increase the efficacy and efficiency of the treatments. Over the last years, a number of image analysis-based methodologies have been proposed for automatic image disease identification. Among these methods, the use of Deep Convolutional Neural Networks (CNNs) has proven tremendously successful for different visual classification tasks.In this work we extend previous work by Johannes et al. (2017) with an adapted Deep Residual Neural Network-based algorithm to deal with the detection of multiple plant diseases in real acquisition conditions where different adaptions for early disease detection have been proposed. This work analyses the performance of early identification of three relevant European endemic wheat diseases: Septoria (Septoria triciti), Tan Spot (Drechslera triciti-repentis) and Rust (Puccinia striiformis &Puccinia recondita). The analysis was done using different mobile devices, and more than 8178 images were captured in two pilot sites in Spain and Germany during 2014,2015 and 2016. Obtained results reveal an overall improvement of the balanced accuracy from 0.78 (Johannes et al., 2017) up to 0.87 under exhaustive testing, and balanced accuracies greater than 0.96 on a pilot test performed in Germany.\n",
      "BACKGROUND: Field phenotyping by remote sensing has received increased interest in recent years with the possibility of achieving high-throughput analysis of crop fields. Along with the various technological developments, the application of machine learning methods for image analysis has enhanced the potential for quantitative assessment of a multitude of crop traits. For wheat breeding purposes, assessing the production of wheat spikes, as the grain-bearing organ, is a useful proxy measure of grain production. Thus, being able to detect and characterize spikes from images of wheat fields is an essential component in a wheat breeding pipeline for the selection of high yielding varieties. RESULTS: We have applied a deep learning approach to accurately detect, count and analyze wheat spikes for yield estimation. We have tested the approach on a set of images of wheat field trial comprising 10 varieties subjected to three fertilizer treatments. The images have been captured over one season, using high definition RGB cameras mounted on a land-based imaging platform, and viewing the wheat plots from an oblique angle. A subset of in-field images has been accurately labeled by manually annotating all the spike regions. This annotated dataset, called SPIKE, is then used to train four region-based Convolutional Neural Networks (R-CNN) which take, as input, images of wheat plots, and accurately detect and count spike regions in each plot. The CNNs also output the spike density and a classification probability for each plot. Using the same R-CNN architecture, four different models were generated based on four different datasets of training and testing images captured at various growth stages. Despite the challenging field imaging conditions, e.g., variable illumination conditions, high spike occlusion, and complex background, the four R-CNN models achieve an average detection accuracy ranging from 88 to [Formula: see text] across different sets of test images. The most robust R-CNN model, which achieved the highest accuracy, is then selected to study the variation in spike production over 10 wheat varieties and three treatments. The SPIKE dataset and the trained CNN are the main contributions of this paper. CONCLUSION: With the availability of good training datasets such us the SPIKE dataset proposed in this article, deep learning techniques can achieve high accuracy in detecting and counting spikes from complex wheat field images. The proposed robust R-CNN model, which has been trained on spike images captured during different growth stages, is optimized for application to a wider variety of field scenarios. It accurately quantifies the differences in yield produced by the 10 varieties we have studied, and their respective responses to fertilizer treatment. We have also observed that the other R-CNN models exhibit more specialized performances. The data set and the R-CNN model, which we make publicly available, have the potential to greatly benefit plant breeders by facilitating the high throughput selection of high yielding varieties.\n",
      "Convolutional Neural Networks (CNN) have demonstrated their capabilities on the agronomical field, especially for plant visual symptoms assessment. As these models grow both in the number of training images and in the number of supported crops and diseases, there exist the dichotomy of (1) generating smaller models for specific crop or, (2) to generate a unique multi-crop model in a much more complex task (especially at early disease stages) but with the benefit of the entire multiple crop image dataset variability to enrich image feature description learning.In this work we first introduce a challenging dataset of more than one hundred-thousand images taken by cell phone in real field wild conditions. This dataset contains almost equally distributed disease stages of seventeen diseases and five crops (wheat, barley, corn, rice and rape-seed) where several diseases can be present on the same picture.When applying existing state of the art deep neural network methods to validate the two hypothesised approaches, we obtained a balanced accuracy (BAC=0.92) when generating the smaller crop specific models and a balanced accuracy (BAC=0.93) when generating a single multi-crop model.In this work, we propose three different CNN architectures that incorporate contextual non-image meta-data such as crop information onto an image based Convolutional Neural Network. This combines the advantages of simultaneously learning from the entire multi-crop dataset while reducing the complexity of the disease classification tasks. The crop-conditional plant disease classification network that incorporates the contextual information by concatenation at the embedding vector level obtains a balanced accuracy of 0.98 improving all previous methods and removing 71% of the miss-classifications of the former methods.\n",
      "The current mainstream approach of using manual measurements and visual inspections for crop lodging detection is inefficient, time-consuming, and subjective. An innovative method for wheat lodging detection that can overcome or alleviate these shortcomings would be welcomed. This study proposed a systematic approach for wheat lodging detection in research plots (372 experimental plots), which consisted of using unmanned aerial systems (UAS) for aerial imagery acquisition, manual field evaluation, and machine learning algorithms to detect the occurrence or not of lodging. UAS imagery was collected on three different dates (23 and 30 July 2019, and 8 August 2019) after lodging occurred. Traditional machine learning and deep learning were evaluated and compared in this study in terms of classification accuracy and standard deviation. For traditional machine learning, five types of features (i.e. gray level co-occurrence matrix, local binary pattern, Gabor, intensity, and Hu-moment) were extracted and fed into three traditional machine learning algorithms (i.e., random forest (RF), neural network, and support vector machine) for detecting lodged plots. For the datasets on each imagery collection date, the accuracies of the three algorithms were not significantly different from each other. For any of the three algorithms, accuracies on the first and last date datasets had the lowest and highest values, respectively. Incorporating standard deviation as a measurement of performance robustness, RF was determined as the most satisfactory. Regarding deep learning, three different convolutional neural networks (simple convolutional neural network, VGG-16, and GoogLeNet) were tested. For any of the single date datasets, GoogLeNet consistently had superior performance over the other two methods. Further comparisons between RF and GoogLeNet demonstrated that the detection accuracies of the two methods were not significantly different from each other (p > 0.05); hence, the choice of any of the two would not affect the final detection accuracies. However, considering the fact that the average accuracy of GoogLeNet (93%) was larger than RF (91%), it was recommended to use GoogLeNet for wheat lodging detection. This research demonstrated that UAS RGB imagery, coupled with the GoogLeNet machine learning algorithm, can be a novel, reliable, objective, simple, low-cost, and effective (accuracy > 90%) tool for wheat lodging detection.\n",
      "Improving the accuracy of edge pixel classification is an important aspect of using convolutional neural networks (CNNs) to extract winter wheat spatial distribution information from remote sensing imagery. In this study, we established a method using prior knowledge obtained from statistical analysis to refine CNN classification results, named post-processing CNN (PP-CNN). First, we used an improved RefineNet model to roughly segment remote sensing imagery in order to obtain the initial winter wheat area and the category probability vector for each pixel. Second, we used manual labels as references and performed statistical analysis on the class probability vectors to determine the filtering conditions and select the pixels that required optimization. Third, based on the prior knowledge that winter wheat pixels were internally similar in color, texture, and other aspects, but different from other neighboring land-use types, the filtered pixels were post-processed to improve the classification accuracy. We used 63 Gaofen-2 images obtained from 2017 to 2019 of a representative Chinese winter wheat region (Feicheng, Shandong Province) to create the dataset and employed RefineNet and SegNet as standard CNN and conditional random field (CRF) as post-process methods, respectively, to conduct comparison experiments. PP-CNN’s accuracy (94.4%), precision (93.9%), and recall (94.4%) were clearly superior, demonstrating its advantages for the improved refinement of edge areas during image classification.\n",
      "Above ground biomass (AGB) is a critical trait indicating the growth of winter wheat. Currently, non-destructive methods for measuring AGB heavily depend on tools such as Remote Sensing and LiDAR, which is subject to specialized knowledge and high-cost. Low-cost solutions appear therefore to be a necessary supplement. In this study, an easy-to-use AGB estimation method for winter wheat at early growth stages was proposed by using digital images captured under field conditions and Deep Convolutional Neural Network (DCNN). Using canopy images as input, the DCNN was trained to learn the relationship between the canopy and the corresponding AGB. To compare the results of the DCNN, conventionally adopted methods for estimating AGB in conjunction with some color and texture feature extraction techniques were used. Results showed strong correlations could be observed between the actual measurements of AGB to those estimated by the DCNN, with high coefficient of determination (R2 = 0.808) and low Root-Mean-Square-Error (RMSE = 0.8913 kg/plot, NRMSE = 24.95%). Factors may influence the accuracy of the DCNN were evaluated. Results showed selecting suitable values of these factors for the DCNN was the guarantee to accurate estimation results. Plant density was proved to be an influence of factor to all the estimation methods based on digital images. The performances of all the methods were influenced to varying degrees while the DCNN achieved the best robustness, indicating the DCNN with RGB images could be an efficient and robust tool for estimating AGB of winter wheat at early growth stages.\n",
      "The growth of most important field crops such as rice, wheat, maize, soybean, and sugarcane are affected due to attack of various pests and the crop production is reduced due to different types of insects. The classification and identification of all types of crop insects correctly is a difficult task for the farmers due to the similar appearance in the earlier stage of crop growth. To address this issue, Convolutional neural network (CNN) with deep architectures is being applied as it performs automatic feature extraction and learns complex high-level features in image classification applications. This study proposed an efficient deep CNN model to classify insect species on three publicly available insect datasets. The National Bureau of Agricultural Insect Resources (NBAIR) dataset used as first insect dataset that consists of 40 classes of field crop insect images, while the second and third dataset (Xie1, Xie2) contains 24 and 40 classes of insects respectively. The proposed model was evaluated and compared with pre-trained deep learning architectures such as AlexNet, ResNet, GoogLeNet and VGGNet for insect classification. Transfer learning was applied to fine-tune the pre-trained models. The data augmentation techniques such as reflection, scaling, rotation, and translation are also applied to prevent the network from overfitting. The effectiveness of hyperparameters was analysed in the proposed model to improve accuracy. The highest classification accuracy of 96.75, 97.47, and 95.97% was achieved in proposed CNN model for NBAIR insect dataset (40 classes), Xie1 (24 classes) insect dataset and Xie2 (40 classes) insect dataset respectively. The results demonstrated that the proposed CNN model is effective in classifying various types of insects in field crops than pre-trained models and can be implemented in the agriculture sector for crop protection.\n"
     ]
    }
   ],
   "source": [
    "for i in range (9):\n",
    "    print(abstract_dict['abstracts'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938\n"
     ]
    }
   ],
   "source": [
    "with open('pubag_abs_dict.json', 'r') as read_file:\n",
    "    abstract_dict = json.load(read_file)\n",
    "print(len(abstract_dict['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
